{
  "category": "Open Source",
  "count": 38,
  "articles": [
    {
      "id": "8974cbf315fc128cbdcbf27fffa994ba",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "How to Transition From Data Analyst to Data Scientist",
      "url": "https://towardsdatascience.com/how-to-transition-from-data-analyst-to-data-scientist/",
      "description": "Playbook on how data analysts can become data scientists\nThe post How to Transition From Data Analyst to Data Scientist appeared first on Towards Data Science.",
      "published_date": "2025-06-09T23:09:55+00:00",
      "collected_at": "2025-06-10T04:25:07.592821+00:00",
      "author": "Egor Howell",
      "source_priority": 3,
      "score": 95
    },
    {
      "id": "5bb2aa15fbc6f8e0fd06eba34a4606b2",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Trying to Stay Sane in the Age of\u00a0AI",
      "url": "https://towardsdatascience.com/trying-to-stay-sane-in-the-age-of-ai/",
      "description": "A machine learning engineer\u2019s quiet way of not losing her mind\nThe post Trying to Stay Sane in the Age of\u00a0AI appeared first on Towards Data Science.",
      "published_date": "2025-06-09T22:50:40+00:00",
      "collected_at": "2025-06-10T04:25:07.593033+00:00",
      "author": "Amy Ma",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "1e574d1ee5929285a8fad62284c44929",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection\n  Behavior",
      "url": "https://arxiv.org/abs/2506.08012",
      "description": "Multimodal Large Language Models (MLLMs) have shown great potential in\nrevolutionizing Graphical User Interface (GUI) automation. However, existing\nGUI models mostly rely on learning from nearly error-free offline trajectories,\nthus lacking reflection and error recovery capabilities. To bridge this gap, we\npropose GUI-Reflection, a novel framework that explicitly integrates\nself-reflection and error correction capabilities into end-to-end multimodal\nGUI models throughout dedicated training stage",
      "published_date": "2025-06-09T13:59:57+00:00",
      "collected_at": "2025-06-10T04:25:05.187385+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "ff065f400d40750a092176da1b225387",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Reinforcement Pre-Training",
      "url": "https://arxiv.org/abs/2506.08007",
      "description": "In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling\nparadigm for large language models and reinforcement learning (RL).\nSpecifically, we reframe next-token prediction as a reasoning task trained\nusing RL, where it receives verifiable rewards for correctly predicting the\nnext token for a given context. RPT offers a scalable method to leverage vast\namounts of text data for general-purpose RL, rather than relying on\ndomain-specific annotated answers. By incentivizing the ca",
      "published_date": "2025-06-09T13:59:53+00:00",
      "collected_at": "2025-06-10T04:25:05.187172+00:00",
      "author": "",
      "source_priority": 1,
      "score": 96
    },
    {
      "id": "2177e4aa7a669ab0c6f1b8925c822a14",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Dreamland: Controllable World Creation with Simulator and Generative\n  Models",
      "url": "https://arxiv.org/abs/2506.08006",
      "description": "Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that enco",
      "published_date": "2025-06-09T13:59:52+00:00",
      "collected_at": "2025-06-10T04:25:05.184151+00:00",
      "author": "",
      "source_priority": 1,
      "score": 96
    },
    {
      "id": "62919ab2d8a7b7284fd9caad39b9c5c1",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers",
      "url": "https://arxiv.org/abs/2506.07986",
      "description": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder th",
      "published_date": "2025-06-09T13:54:04+00:00",
      "collected_at": "2025-06-10T04:25:05.184671+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "313ca2f1219e74dece72fc76fc16ab1d",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation",
      "url": "https://arxiv.org/abs/2506.07977",
      "description": "Text-to-image (T2I) models have garnered significant attention for generating\nhigh-quality images aligned with text prompts. However, rapid T2I model\nadvancements reveal limitations in early benchmarks, lacking comprehensive\nevaluations, for example, the evaluation on reasoning, text rendering and\nstyle. Notably, recent state-of-the-art models, with their rich knowledge\nmodeling capabilities, show promising results on the image generation problems\nrequiring strong reasoning ability, yet existing",
      "published_date": "2025-06-09T13:50:21+00:00",
      "collected_at": "2025-06-10T04:25:05.186056+00:00",
      "author": "",
      "source_priority": 1,
      "score": 96
    },
    {
      "id": "e1030e3c4ee1032ddae49334dc3c8dab",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
      "url": "https://arxiv.org/abs/2506.07900",
      "description": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we pro",
      "published_date": "2025-06-09T12:16:50+00:00",
      "collected_at": "2025-06-10T04:25:05.186470+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "f188e81e6748cfa276498f0b3f3a1b17",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Through the Valley: Path to Effective Long CoT Training for Small\n  Language Models",
      "url": "https://arxiv.org/abs/2506.07712",
      "description": "Long chain-of-thought (CoT) supervision has become a common strategy to\nenhance reasoning in language models. While effective for large models, we\nidentify a phenomenon we call Long CoT Degradation, in which small language\nmodels (SLMs; <=3B parameters) trained on limited long CoT data experience\nsignificant performance deterioration. Through extensive experiments on the\nQwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is\nwidespread across SLMs. In some settings, models",
      "published_date": "2025-06-09T08:56:41+00:00",
      "collected_at": "2025-06-10T04:25:05.187615+00:00",
      "author": "",
      "source_priority": 1,
      "score": 95
    },
    {
      "id": "38cc25aeb4ec399c70e1f1c013a52e4f",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular\n  Structure Recognition",
      "url": "https://arxiv.org/abs/2506.07553",
      "description": "Optical Chemical Structure Recognition (OCSR) is crucial for digitizing\nchemical knowledge by converting molecular images into machine-readable\nformats. While recent vision-language models (VLMs) have shown potential in\nthis task, their image-captioning approach often struggles with complex\nmolecular structures and inconsistent annotations. To overcome these\nchallenges, we introduce GTR-Mol-VLM, a novel framework featuring two key\ninnovations: (1) the Graph Traversal as Visual Chain of Thought\nm",
      "published_date": "2025-06-09T04:47:10+00:00",
      "collected_at": "2025-06-10T04:25:05.186701+00:00",
      "author": "",
      "source_priority": 1,
      "score": 95
    },
    {
      "id": "3ae7f3521a3980ca495c44db8b8d3f9b",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation",
      "url": "https://arxiv.org/abs/2506.07530",
      "description": "Vision-Language-Action (VLA) models have shown impressive capabilities across\na wide range of robotics manipulation tasks. However, their growing model size\nposes significant challenges for deployment on resource-constrained robotic\nsystems. While 1-bit pretraining has proven effective for enhancing the\ninference efficiency of large language models with minimal performance loss,\nits application to VLA models remains underexplored. In this work, we present\nBitVLA, the first 1-bit VLA model for ro",
      "published_date": "2025-06-09T04:15:11+00:00",
      "collected_at": "2025-06-10T04:25:05.187826+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "a8ca031f13201575b3180a672ea9a95b",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling",
      "url": "https://arxiv.org/abs/2506.07491",
      "description": "SpatialLM is a large language model designed to process 3D point cloud data\nand generate structured 3D scene understanding outputs. These outputs include\narchitectural elements like walls, doors, windows, and oriented object boxes\nwith their semantic categories. Unlike previous methods which exploit\ntask-specific network designs, our model adheres to the standard multimodal LLM\narchitecture and is fine-tuned directly from open-source LLMs.\n  To train SpatialLM, we collect a large-scale, high-qua",
      "published_date": "2025-06-09T03:10:58+00:00",
      "collected_at": "2025-06-10T04:25:05.185138+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "a1e0690ebabe294c04f5d1ce1c489c20",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large\n  Language Models",
      "url": "https://arxiv.org/abs/2506.07463",
      "description": "We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered\nfor superior data quality and diverse human-like reasoning trajectory. CCI4.0\noccupies roughly 35 TB of disk space and comprises two sub-datasets:\nCCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a 5.2 TB carefully\ncurated Chinese web corpus, a 22.5 TB English subset from Nemotron-CC, and\ndiverse sources from math, wiki, arxiv, and code. Although these data are\nmostly sourced from well-processed datasets, the q",
      "published_date": "2025-06-09T02:14:19+00:00",
      "collected_at": "2025-06-10T04:25:05.184918+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "52c6f6755236133f8929f23bf407b287",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Well Begun is Half Done: Low-resource Preference Alignment by\n  Weak-to-Strong Decoding",
      "url": "https://arxiv.org/abs/2506.07434",
      "description": "Large Language Models (LLMs) require alignment with human preferences to\navoid generating offensive, false, or meaningless content. Recently,\nlow-resource methods for LLM alignment have been popular, while still facing\nchallenges in obtaining both high-quality and aligned content. Motivated by the\nobservation that the difficulty of generating aligned responses is concentrated\nat the beginning of decoding, we propose a novel framework, Weak-to-Strong\nDecoding (WSD), to enhance the alignment abili",
      "published_date": "2025-06-09T01:21:22+00:00",
      "collected_at": "2025-06-10T04:25:05.185842+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "542c25c98748b4f907f12c60caa2ab90",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Pre-trained Large Language Models Learn Hidden Markov Models In-context",
      "url": "https://arxiv.org/abs/2506.07298",
      "description": "Hidden Markov Models (HMMs) are foundational tools for modeling sequential\ndata with latent Markovian structure, yet fitting them to real-world data\nremains computationally challenging. In this work, we show that pre-trained\nlarge language models (LLMs) can effectively model data generated by HMMs via\nin-context learning (ICL)x2013their ability to infer patterns from\nexamples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve\npredictive accuracy approaching the theoretical optimum",
      "published_date": "2025-06-08T17:49:38+00:00",
      "collected_at": "2025-06-10T04:25:05.185362+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "4a4852d702fb491a7018af39d73205fa",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\n  Understanding and Reasoning",
      "url": "https://arxiv.org/abs/2506.07044",
      "description": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge be",
      "published_date": "2025-06-08T04:47:30+00:00",
      "collected_at": "2025-06-10T04:25:05.186915+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "53d53caaf4f8bd29e627a1071cfec4c9",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of\n  Reasoning Models via the Lens of Problem Complexity",
      "url": "https://arxiv.org/abs/2506.06941",
      "description": "Recent generations of language models have introduced Large Reasoning Models\n(LRMs) that generate detailed thinking processes before providing answers.\nWhile these models demonstrate improved performance on reasoning benchmarks,\ntheir fundamental capabilities, scaling properties, and limitations remain\ninsufficiently understood. Current evaluations primarily focus on established\nmath and coding benchmarks, emphasizing final answer accuracy. However, this\nevaluation paradigm often suffers from co",
      "published_date": "2025-06-07T18:42:29+00:00",
      "collected_at": "2025-06-10T04:25:05.185586+00:00",
      "author": "",
      "source_priority": 1,
      "score": 93
    },
    {
      "id": "ba0a1d0ca488df04266d6f48441d1523",
      "source_id": "pytorch_blog",
      "source": "Blog \u2013 PyTorch",
      "category": "Open Source",
      "title": "HuggingFace Safetensors Support in PyTorch Distributed Checkpointing",
      "url": "https://pytorch.org/blog/huggingface-safetensors-support-in-pytorch-distributed-checkpointing/",
      "description": "Summary\u00a0 PyTorch Distributed Checkpointing (DCP) is making investments into addressing the interoperability blockers to ensure that popular formats, like HuggingFace safetensors, can work well with PyTorch\u2019s ecosystem. Since HuggingFace has...",
      "published_date": "2025-06-06T19:17:46+00:00",
      "collected_at": "2025-06-10T04:25:05.400502+00:00",
      "author": "Ankita George, Saurabh Mishra, Joe Cummings, Philip Bontrager, Daulet Askarov, Teja Rao, Chien-Chin Huang, Ela Krepska, Jafar Taghiyar",
      "source_priority": 2,
      "score": 92
    },
    {
      "id": "f2e2ef796ae85073f0397c059bcd40ea",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Not Everything Needs Automation: 5 Practical AI Agents That Deliver Enterprise Value",
      "url": "https://towardsdatascience.com/not-everything-needs-automation-5-practical-ai-agents-that-deliver-enterprise-value/",
      "description": "What actually works with AI agents inside enterprise organizations?\nThe post Not Everything Needs Automation: 5 Practical AI Agents That Deliver Enterprise Value appeared first on Towards Data Science.",
      "published_date": "2025-06-06T17:54:12+00:00",
      "collected_at": "2025-06-10T04:25:07.593235+00:00",
      "author": "Weiwei Hu",
      "source_priority": 3,
      "score": 91
    },
    {
      "id": "3d5ee6adb52013963931f8bccef6033c",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Prescriptive Modeling Unpacked: A Complete Guide to Intervention With Bayesian Modeling.",
      "url": "https://towardsdatascience.com/prescriptive-modeling-unpacked-a-complete-guide-to-intervention-with-bayesian-modeling/",
      "description": "Learn how to move beyond prediction and actively make intervention through prescriptive modeling. This in-depth guide walks you through Bayesian approaches to system intervention, with practical examples in predictive maintenance.\nThe post Prescriptive Modeling Unpacked: A Complete Guide to Intervention With Bayesian Modeling. appeared first on Towards Data Science.",
      "published_date": "2025-06-06T17:23:32+00:00",
      "collected_at": "2025-06-10T04:25:07.593431+00:00",
      "author": "Erdogan Taskesen",
      "source_priority": 3,
      "score": 91
    },
    {
      "id": "cbb488da25d5e23528a79c1c8cdd9eaf",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance",
      "url": "https://arxiv.org/abs/2506.06444",
      "description": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that co",
      "published_date": "2025-06-06T14:05:45+00:00",
      "collected_at": "2025-06-10T04:25:05.184405+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "1baeabdb760d5d46d8825a522a214e3e",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "How I Automated My Machine Learning Workflow with Just 10 Lines of\u00a0Python",
      "url": "https://towardsdatascience.com/how-i-automated-my-machine-learning-workflow-with-just-10-lines-of-python/",
      "description": "Use LazyPredict and PyCaret to skip the grunt work and jump straight to performance.\nThe post How I Automated My Machine Learning Workflow with Just 10 Lines of\u00a0Python appeared first on Towards Data Science.",
      "published_date": "2025-06-06T13:11:46+00:00",
      "collected_at": "2025-06-10T04:25:07.593647+00:00",
      "author": "Himanshu Sharma",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "189d9a77fa5b000dafc2b8d8a009696c",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "The Role of Luck in Sports: Can We Measure It?",
      "url": "https://towardsdatascience.com/the-role-of-luck-in-sports-can-we-measure-it/",
      "description": "From last-minute goals to coin tosses: How much does randomness influence the outcomes of games?\nThe post The Role of Luck in Sports: Can We Measure It? appeared first on Towards Data Science.",
      "published_date": "2025-06-06T12:56:13+00:00",
      "collected_at": "2025-06-10T04:25:07.593849+00:00",
      "author": "Pol Marin",
      "source_priority": 3,
      "score": 90
    },
    {
      "id": "13273eb8ef106c023f296d5889a2003f",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Why AI Projects Fail",
      "url": "https://towardsdatascience.com/why-ai-projects-fail/",
      "description": "No one agrees on the exact number, but estimates say anywhere from 50% to 80% of AI projects end in failure.\nThe post Why AI Projects Fail appeared first on Towards Data Science.",
      "published_date": "2025-06-06T12:49:02+00:00",
      "collected_at": "2025-06-10T04:25:07.594042+00:00",
      "author": "Ivo Bernardo",
      "source_priority": 3,
      "score": 90
    },
    {
      "id": "cc9ddd43ece4c0107e3dc3a3caa42148",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "5 Crucial Tweaks That Will Make Your Charts Accessible to People with Visual Impairments",
      "url": "https://towardsdatascience.com/5-crucial-tweaks-that-will-make-your-charts-accessible-to-people-with-visual-impairments/",
      "description": "More than 350 million people are colorblind - Make sure they can read your visualizations.\nThe post 5 Crucial Tweaks That Will Make Your Charts Accessible to People with Visual Impairments appeared first on Towards Data Science.",
      "published_date": "2025-06-06T12:39:46+00:00",
      "collected_at": "2025-06-10T04:25:07.594233+00:00",
      "author": "Dario Rade\u010di\u0107",
      "source_priority": 3,
      "score": 90
    },
    {
      "id": "d7545a1a922f41c500ffbeb3b257c62d",
      "source_id": "pytorch_blog",
      "source": "Blog \u2013 PyTorch",
      "category": "Open Source",
      "title": "Introducing the PyTorch Ecosystem Working Group and Project Spotlights",
      "url": "https://pytorch.org/blog/introducing-the-pytorch-ecosystem-working-group-and-project-spotlights/",
      "description": "The PyTorch Ecosystem goes back several years, with some of its earliest projects like Hugging Face, Fast.ai, and PyTorch Lightning going on to grow incredible communities of their own. The...",
      "published_date": "2025-06-05T19:57:09+00:00",
      "collected_at": "2025-06-10T04:25:05.400583+00:00",
      "author": "PyTorch Ecosystem Working Group",
      "source_priority": 2,
      "score": 91
    },
    {
      "id": "5259b44ad217237ed86c4664ffeb78eb",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "When Semantics Mislead Vision: Mitigating Large Multimodal Models\n  Hallucinations in Scene Text Spotting and Understanding",
      "url": "https://arxiv.org/abs/2506.05551",
      "description": "Large Multimodal Models (LMMs) have achieved impressive progress in visual\nperception and reasoning. However, when confronted with visually ambiguous or\nnon-semantic scene text, they often struggle to accurately spot and understand\nthe content, frequently generating semantically plausible yet visually\nincorrect answers, which we refer to as semantic hallucination. In this work,\nwe investigate the underlying causes of semantic hallucination and identify a\nkey finding: Transformer layers in LLM wi",
      "published_date": "2025-06-05T15:53:19+00:00",
      "collected_at": "2025-06-10T04:25:05.188305+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "07911bf4783ae8c988df398024f0a29d",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "The Journey from Jupyter to Programmer: A Quick-Start Guide",
      "url": "https://towardsdatascience.com/the-journey-from-jupyter-to-programmer-a-quick-start-guide/",
      "description": "Explore the real benefits of ditching the notebook\nThe post The Journey from Jupyter to Programmer: A Quick-Start Guide appeared first on Towards Data Science.",
      "published_date": "2025-06-04T23:22:34+00:00",
      "collected_at": "2025-06-10T04:25:07.594424+00:00",
      "author": "Lucy Dickinson",
      "source_priority": 3,
      "score": 88
    },
    {
      "id": "6a6aa23d24a1e4c8a12c3043c7f51740",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Building a Modern Dashboard with Python and Gradio",
      "url": "https://towardsdatascience.com/building-a-modern-dashboard-with-python-and-gradio/",
      "description": "Data insights made simple\nThe post Building a Modern Dashboard with Python and Gradio appeared first on Towards Data Science.",
      "published_date": "2025-06-04T22:33:11+00:00",
      "collected_at": "2025-06-10T04:25:07.594631+00:00",
      "author": "Thomas Reid",
      "source_priority": 3,
      "score": 88
    },
    {
      "id": "b1b6d73dcf6dbf65fecf7f79e99c8711",
      "source_id": "pytorch_blog",
      "source": "Blog \u2013 PyTorch",
      "category": "Open Source",
      "title": "Open Source AI is Transforming the Economy\u2014Here\u2019s What the Data Shows",
      "url": "https://pytorch.org/blog/open-source-ai-is-transforming-the-economy-heres-what-the-data-shows/",
      "description": "Blog cross-posted on the Linux Foundation blog.\nAs we approach the midpoint of 2025, the potential of AI to transform businesses, economies, and industries is not only widely anticipated and nearly universal but also well documented. In a commissioned project by Meta, LF Research set out to capture existing evidence on this topic, with the specific aim of understanding how open source is playing a role in this transformation.\nIn its latest publication,\u00a0The Economic and Workforce Impacts o",
      "published_date": "2025-06-04T19:31:06+00:00",
      "collected_at": "2025-06-10T04:25:05.402989+00:00",
      "author": "Frank Nagle, Assistant Professor in the Strategy Unit at Harvard Business School and Advising Chief Economist at the Linux Foundation",
      "source_priority": 2,
      "score": 100
    },
    {
      "id": "6e80dcb1096a6d87c7d198fb883f6e73",
      "source_id": "pytorch_blog",
      "source": "Blog \u2013 PyTorch",
      "category": "Open Source",
      "title": "Build Responsible AI Products with your own Yellow Teaming LLM",
      "url": "https://pytorch.org/blog/build-responsible-ai-products-with-your-own-yellow-teaming-llm/",
      "description": "The tools we use to build AI are evolving fast, with PyTorch at the heart of many advances. But unless we evolve the way we approach building AI systems, we...",
      "published_date": "2025-06-04T14:37:23+00:00",
      "collected_at": "2025-06-10T04:25:05.403071+00:00",
      "author": "Zach Lasiuk, Principal Solutions Designer, Arm",
      "source_priority": 2,
      "score": 100
    },
    {
      "id": "d7a51dbda5e3adf0145c0acfb93f4c8e",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Robust Preference Optimization via Dynamic Target Margins",
      "url": "https://arxiv.org/abs/2506.03690",
      "description": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose gamma-PO, a\ndynamic target margin preference optimization algorithm",
      "published_date": "2025-06-04T04:19:37+00:00",
      "collected_at": "2025-06-10T04:25:05.186263+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "aeb2a3a5b979436d7a4d638f220bc43a",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Data Drift Is Not the Actual Problem: Your Monitoring Strategy Is",
      "url": "https://towardsdatascience.com/data-drift-is-not-the-actual-problem-your-monitoring-strategy-is/",
      "description": "Monitoring is easy; what to monitor is not. In the field of machine learning, data drift is\njust noise until you know what it means.\nThe post Data Drift Is Not the Actual Problem: Your Monitoring Strategy Is appeared first on Towards Data Science.",
      "published_date": "2025-06-03T23:24:53+00:00",
      "collected_at": "2025-06-10T04:25:07.594848+00:00",
      "author": "Mahe Jabeen Abdul",
      "source_priority": 3,
      "score": 99
    },
    {
      "id": "2d6cb93f255f8f2835621f7477c42cae",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Reducing Time to Value for Data Science Projects: Part 2",
      "url": "https://towardsdatascience.com/reducing-time-to-value-for-data-science-projects-part-2/",
      "description": "Leveraging automation and parallelism to scale out experiments\nThe post Reducing Time to Value for Data Science Projects: Part 2 appeared first on Towards Data Science.",
      "published_date": "2025-06-03T23:18:14+00:00",
      "collected_at": "2025-06-10T04:25:07.595037+00:00",
      "author": "Kristopher McGlinchey",
      "source_priority": 3,
      "score": 97
    },
    {
      "id": "3b55e34b8685702b557fb90455c481a6",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Pairwise Cross-Variance Classification",
      "url": "https://towardsdatascience.com/pairwise-cross-variance-classification/",
      "description": "Multi-class zero-shot embedding classification and error checking\nThe post Pairwise Cross-Variance Classification appeared first on Towards Data Science.",
      "published_date": "2025-06-03T19:32:49+00:00",
      "collected_at": "2025-06-10T04:25:07.595228+00:00",
      "author": "Doster Esh",
      "source_priority": 3,
      "score": 87
    },
    {
      "id": "7ed05f9c1c2f8bd5caec95c6b41a28d7",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Landing your First Machine Learning Job: Startup vs Big Tech vs Academia",
      "url": "https://towardsdatascience.com/landing-your-first-machine-learning-job-startup-vs-big-tech-vs-academia/",
      "description": "A practical guide to landing your first Machine Learning job across startups, big tech, and academia.\nThe post Landing your First Machine Learning Job: Startup vs Big Tech vs Academia appeared first on Towards Data Science.",
      "published_date": "2025-06-03T19:27:55+00:00",
      "collected_at": "2025-06-10T04:25:07.595418+00:00",
      "author": "Piero Paialunga",
      "source_priority": 3,
      "score": 99
    },
    {
      "id": "d2f00b54f8f60670a623b19b6f330c5d",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Decision Trees Natively Handle Categorical Data",
      "url": "https://towardsdatascience.com/decision-trees-natively-handle-categorical-data/",
      "description": "But mean target encoding is their turbocharger\nThe post Decision Trees Natively Handle Categorical Data appeared first on Towards Data Science.",
      "published_date": "2025-06-03T18:27:12+00:00",
      "collected_at": "2025-06-10T04:25:07.595620+00:00",
      "author": "Vadim Arzamasov",
      "source_priority": 3,
      "score": 87
    },
    {
      "id": "944ad331646feb41fcb25a11433a7056",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "How to Design My First AI Agent",
      "url": "https://towardsdatascience.com/how-to-design-my-first-ai-agent/",
      "description": "The foundations of designing an AI agent\nThe post How to Design My First AI Agent appeared first on Towards Data Science.",
      "published_date": "2025-06-03T18:19:29+00:00",
      "collected_at": "2025-06-10T04:25:07.595812+00:00",
      "author": "Fabiana Clemente",
      "source_priority": 3,
      "score": 87
    }
  ]
}