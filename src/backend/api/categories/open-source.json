{
  "category": "Open Source",
  "count": 38,
  "articles": [
    {
      "id": "63a113be7db3662f6d0529a921d6e3c7",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Abstract Classes: A Software Engineering Concept Data Scientists Must Know To Succeed",
      "url": "https://towardsdatascience.com/abstract-classes-a-software-engineering-concept-data-scientists-must-know-to-succeed/",
      "description": "Simple concepts that differentiate a professional from amateurs.\nThe post Abstract Classes: A Software Engineering Concept Data Scientists Must Know To Succeed appeared first on Towards Data Science.",
      "published_date": "2025-06-17T22:45:07+00:00",
      "collected_at": "2025-06-18T01:19:37.350554+00:00",
      "author": "Benjamin Lee",
      "source_priority": 3,
      "score": 95
    },
    {
      "id": "e518c9d1e51cfbf728fff80b3577258d",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "LLaVA on a Budget: Multimodal AI with Limited Resources",
      "url": "https://towardsdatascience.com/llava-on-a-budget-multimodal-ai-with-limited-resources/",
      "description": "Let's get started with multimodality\nThe post LLaVA on a Budget: Multimodal AI with Limited Resources appeared first on Towards Data Science.",
      "published_date": "2025-06-17T18:06:11+00:00",
      "collected_at": "2025-06-18T01:19:37.350763+00:00",
      "author": "Marcello Politi",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "46fbbad38f9c13bbcad1ecdf41de7410",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Apply Sphinx’s Functionality to Create Documentation for Your Next Data Science Project",
      "url": "https://towardsdatascience.com/apply-sphinxs-functionality-to-create-documentation-for-your-next-data-science-project/",
      "description": "Three cases to use the Sphinx tool as a pro\nThe post Apply Sphinx’s Functionality to Create Documentation for Your Next Data Science Project appeared first on Towards Data Science.",
      "published_date": "2025-06-17T17:55:42+00:00",
      "collected_at": "2025-06-18T01:19:37.350973+00:00",
      "author": "Radmila Mandzhieva",
      "source_priority": 3,
      "score": 95
    },
    {
      "id": "23535b2f4318b27f71fcd30c6192cbee",
      "source_id": "pytorch_blog",
      "source": "Blog – PyTorch",
      "category": "Open Source",
      "title": "DeepNVMe: Affordable I/O scaling for Deep Learning Applications",
      "url": "https://pytorch.org/blog/deepnvme-affordable-i-o-scaling-for-deep-learning-applications/",
      "description": "Introduction We introduced DeepNVMe in summer 2024 as a suite of optimizations for tackling I/O bottlenecks in Deep Learning (DL). DeepNVMe delivers significant speedups for I/O bound DL workloads by leveraging storage...",
      "published_date": "2025-06-17T16:54:32+00:00",
      "collected_at": "2025-06-18T01:19:36.249278+00:00",
      "author": "Joe Mayer, Logan Adams, Olatunji Ruwase",
      "source_priority": 2,
      "score": 100
    },
    {
      "id": "b148d646bfb135a6466ca6f9950f0689",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Grad-CAM from Scratch with PyTorch Hooks",
      "url": "https://towardsdatascience.com/grad-cam-from-scratch-with-pytorch-hooks/",
      "description": "A hands-on look at an explainable AI (XAI) technique that helps reveal why a convolutional neural network (CNN) made a particular decision\nThe post Grad-CAM from Scratch with PyTorch Hooks appeared first on Towards Data Science.",
      "published_date": "2025-06-17T05:52:18+00:00",
      "collected_at": "2025-06-18T01:19:37.351196+00:00",
      "author": "Conor O'Sullivan",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "1fb2693d4ea1e39ad8933b756d329ea3",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "DiffusionBlocks: Blockwise Training for Generative Models via\n  Score-Based Diffusion",
      "url": "https://arxiv.org/abs/2506.14202",
      "description": "Training large neural networks with end-to-end backpropagation creates\nsignificant memory bottlenecks, limiting accessibility to state-of-the-art AI\nresearch. We propose DiffusionBlocks, a novel training framework\nthat interprets neural network blocks as performing denoising operations in a\ncontinuous-time diffusion process. By partitioning the network into\nindependently trainable blocks and optimizing noise level assignments based on\nequal cumulative probability mass, our approach achieves sign",
      "published_date": "2025-06-17T01:44:18+00:00",
      "collected_at": "2025-06-18T01:19:36.055559+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "32d314415eaab25648b9084330e0895e",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "A Practical Starters’ Guide to Causal Structure Learning with Bayesian Methods in Python",
      "url": "https://towardsdatascience.com/a-practical-starters-guide-to-causal-structure-learning-with-bayesian-methods-in-python/",
      "description": "Learn Causal Structures and make inferences with Bayesian Methods: Python Tutorial\nThe post A Practical Starters’ Guide to Causal Structure Learning with Bayesian Methods in Python appeared first on Towards Data Science.",
      "published_date": "2025-06-17T00:45:46+00:00",
      "collected_at": "2025-06-18T01:19:37.351400+00:00",
      "author": "Erdogan Taskesen",
      "source_priority": 3,
      "score": 94
    },
    {
      "id": "8f00cd1d07b38b480a6e2f527120fa48",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Essential-Web v1.0: 24T tokens of organized web data",
      "url": "https://arxiv.org/abs/2506.14111",
      "description": "Data plays the most prominent role in how language models acquire skills and\nknowledge. The lack of massive, well-organized pre-training datasets results in\ncostly and inaccessible data pipelines. We present Essential-Web v1.0, a\n24-trillion-token dataset in which every document is annotated with a\ntwelve-category taxonomy covering topic, format, content complexity, and\nquality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned\n0.5b-parameter model that achieves an annotator agreeme",
      "published_date": "2025-06-16T22:03:36+00:00",
      "collected_at": "2025-06-18T01:19:36.055850+00:00",
      "author": "",
      "source_priority": 1,
      "score": 95
    },
    {
      "id": "3b18452ba89445f24ebc15f969f67868",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Let’s Analyze OpenAI’s Claims About ChatGPT Energy Use",
      "url": "https://towardsdatascience.com/lets-analyze-openais-claims-about-chatgpt-energy-use/",
      "description": "ChatGPT uses an average of 0.34 Wh per query, according to a blog post by Sam Altman. Does that figure hold up?\nThe post Let’s Analyze OpenAI’s Claims About ChatGPT Energy Use appeared first on Towards Data Science.",
      "published_date": "2025-06-16T19:31:58+00:00",
      "collected_at": "2025-06-18T01:19:37.351600+00:00",
      "author": "Kasper Groes Albin Ludvigsen",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "8d8888a67d52c0320ffd86cbc05ed4cb",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Regularisation: A Deep Dive into Theory, Implementation, and Practical Insights",
      "url": "https://towardsdatascience.com/regularisation-a-deep-dive-into-theory-implementation-and-practical-insights/",
      "description": "A detailed guide on controlling overfitting and increasing the stability of your models.\nThe post Regularisation: A Deep Dive into Theory, Implementation, and Practical Insights appeared first on Towards Data Science.",
      "published_date": "2025-06-16T19:16:38+00:00",
      "collected_at": "2025-06-18T01:19:37.351796+00:00",
      "author": "Sourav Mohile",
      "source_priority": 3,
      "score": 94
    },
    {
      "id": "a0092c46510f51961c64ac45063c699d",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Build an AI Agent to Explore Your Data Catalog with Natural Language",
      "url": "https://towardsdatascience.com/build-and-ai-agent-to-explore-your-data-catalog-with-natural-language/",
      "description": "Leverage LLMs to query your Databricks Data Catalog\nThe post Build an AI Agent to Explore Your Data Catalog with Natural Language appeared first on Towards Data Science.",
      "published_date": "2025-06-16T17:37:42+00:00",
      "collected_at": "2025-06-18T01:19:37.352004+00:00",
      "author": "Fabiana Clemente",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "38317db47248e6c73c6016a6ca7a648a",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "I Won $10,000 in a Machine Learning Competition — Here’s My Complete Strategy",
      "url": "https://towardsdatascience.com/i-won-10000-in-a-machine-learning-competition-heres-my-complete-strategy/",
      "description": "Complete guide to feature selection, threshold optimization, and neural network architecture for ML competitions\nThe post I Won $10,000 in a Machine Learning Competition — Here’s My Complete Strategy appeared first on Towards Data Science.",
      "published_date": "2025-06-16T17:12:24+00:00",
      "collected_at": "2025-06-18T01:19:37.352207+00:00",
      "author": "Claudia Ng",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "c3ad6fbaec8be7e9f6249a3c8589b100",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Agents, APIs, and the Next Layer of the Internet",
      "url": "https://towardsdatascience.com/agents-apis-and-the-next-layer-of-the-internet/",
      "description": "Part I: Shipping Containers for Thought Every so often a simple idea rewires everything. The shipping container didn’t just optimise logistics; it flattened the globe, collapsed time zones, and rewrote the economics of trade. In its geometric austerity was a quiet revolution: standardisation. Similarly, HTML and HTTP didn’t invent information exchange — any more than […]\nThe post Agents, APIs, and the Next Layer of the Internet appeared first on Towards Data Science.",
      "published_date": "2025-06-16T17:06:00+00:00",
      "collected_at": "2025-06-18T01:19:37.352417+00:00",
      "author": "Cooper Doyle",
      "source_priority": 3,
      "score": 94
    },
    {
      "id": "0d758bf212305a8ba8f0a3dfb75fade1",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "BOW: Bottlenecked Next Word Exploration",
      "url": "https://arxiv.org/abs/2506.13502",
      "description": "Large language models (LLMs) are typically trained via next-word prediction\n(NWP), which provides strong surface-level fluency but often lacks support for\nrobust reasoning. We propose BOttlenecked next Word exploration (BOW), a novel\nRL framework that rethinks NWP by introducing a reasoning bottleneck where a\npolicy model first generates a reasoning path rather than predicting the next\ntoken directly, after which a frozen judge model predicts the next token\ndistribution based solely on this reas",
      "published_date": "2025-06-16T09:58:54+00:00",
      "collected_at": "2025-06-18T01:19:36.056145+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "a43379776775560cc57eeadfb0e0e95b",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Uncertainty-Aware Remaining Lifespan Prediction from Images",
      "url": "https://arxiv.org/abs/2506.13430",
      "description": "Predicting mortality-related outcomes from images offers the prospect of\naccessible, noninvasive, and scalable health screening. We present a method\nthat leverages pretrained vision transformer foundation models to estimate\nremaining lifespan from facial and whole-body images, alongside robust\nuncertainty quantification. We show that predictive uncertainty varies\nsystematically with the true remaining lifespan, and that this uncertainty can\nbe effectively modeled by learning a Gaussian distribut",
      "published_date": "2025-06-16T08:47:37+00:00",
      "collected_at": "2025-06-18T01:19:36.059064+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "e43c681885fdf5cd12e38ba0df17531b",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "A Technical Study into Small Reasoning Language Models",
      "url": "https://arxiv.org/abs/2506.13404",
      "description": "The ongoing evolution of language models has led to the development of\nlarge-scale architectures that demonstrate exceptional performance across a\nwide range of tasks. However, these models come with significant computational\nand energy demands, as well as potential privacy implications. In this context,\nSmall Reasoning Language Models (SRLMs) with approximately 0.5 billion\nparameters present a compelling alternative due to their remarkable\ncomputational efficiency and cost effectiveness, partic",
      "published_date": "2025-06-16T08:18:11+00:00",
      "collected_at": "2025-06-18T01:19:36.057814+00:00",
      "author": "",
      "source_priority": 1,
      "score": 94
    },
    {
      "id": "a864c874d52c01c578a683d48e087256",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT\n  and RL Synergy",
      "url": "https://arxiv.org/abs/2506.13284",
      "description": "In this work, we investigate the synergy between supervised fine-tuning (SFT)\nand reinforcement learning (RL) in developing strong reasoning models. We begin\nby curating the SFT training data through two scaling strategies: increasing\nthe number of collected prompts and the number of generated responses per\nprompt. Both approaches yield notable improvements in reasoning performance,\nwith scaling the number of prompts resulting in more substantial gains. We then\nexplore the following questions re",
      "published_date": "2025-06-16T05:27:48+00:00",
      "collected_at": "2025-06-18T01:19:36.056667+00:00",
      "author": "",
      "source_priority": 1,
      "score": 94
    },
    {
      "id": "277c5bc7371016fb6be32bfaa4794c34",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "SeqPE: Transformer with Sequential Position Encoding",
      "url": "https://arxiv.org/abs/2506.13277",
      "description": "Since self-attention layers in Transformers are permutation invariant by\ndesign, positional encodings must be explicitly incorporated to enable spatial\nunderstanding. However, fixed-size lookup tables used in traditional learnable\nposition embeddings (PEs) limit extrapolation capabilities beyond pre-trained\nsequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this\nlimitation but demand extensive modifications for adapting to new modalities,\nunderscoring fundamental challeng",
      "published_date": "2025-06-16T05:16:40+00:00",
      "collected_at": "2025-06-18T01:19:36.057576+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "5505de3305e167a9da28b730c7f516cf",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Personalizable Long-Context Symbolic Music Infilling with MIDI-RWKV",
      "url": "https://arxiv.org/abs/2506.13001",
      "description": "Existing work in automatic music generation has primarily focused on\nend-to-end systems that produce complete compositions or continuations.\nHowever, because musical composition is typically an iterative process, such\nsystems make it difficult to engage in the back-and-forth between human and\nmachine that is essential to computer-assisted creativity. In this study, we\naddress the task of personalizable, multi-track, long-context, and controllable\nsymbolic music infilling to enhance the process o",
      "published_date": "2025-06-15T20:04:01+00:00",
      "collected_at": "2025-06-18T01:19:36.055284+00:00",
      "author": "",
      "source_priority": 1,
      "score": 93
    },
    {
      "id": "6a2d03fe9611a07321cac4f022dd0ebf",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "DoTA-RAG: Dynamic of Thought Aggregation RAG",
      "url": "https://arxiv.org/abs/2506.12571",
      "description": "In this paper, we introduce DoTA-RAG (Dynamic-of-Thought Aggregation RAG), a\nretrieval-augmented generation system optimized for high-throughput,\nlarge-scale web knowledge indexes. Traditional RAG pipelines often suffer from\nhigh latency and limited accuracy over massive, diverse datasets. DoTA-RAG\naddresses these challenges with a three-stage pipeline: query rewriting,\ndynamic routing to specialized sub-indexes, and multi-stage retrieval and\nranking. We further enhance retrieval by evaluating a",
      "published_date": "2025-06-14T12:56:00+00:00",
      "collected_at": "2025-06-18T01:19:36.058315+00:00",
      "author": "",
      "source_priority": 1,
      "score": 92
    },
    {
      "id": "1213bd69e720ecc0d98b7b5cd50173f9",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Profiling News Media for Factuality and Bias Using LLMs and the\n  Fact-Checking Methodology of Human Experts",
      "url": "https://arxiv.org/abs/2506.12552",
      "description": "In an age characterized by the proliferation of mis- and disinformation\nonline, it is critical to empower readers to understand the content they are\nreading. Important efforts in this direction rely on manual or automatic\nfact-checking, which can be challenging for emerging claims with limited\ninformation. Such scenarios can be handled by assessing the reliability and the\npolitical bias of the source of the claim, i.e., characterizing entire news\noutlets rather than individual claims or articles",
      "published_date": "2025-06-14T11:49:20+00:00",
      "collected_at": "2025-06-18T01:19:36.058558+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "0982c8bdf5f0c9191bd5ac8f4ecf7bf5",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Stop Building AI Platforms",
      "url": "https://towardsdatascience.com/stop-building-ai-platforms/",
      "description": "When small and medium companies achieve success in building Data and ML platforms, building AI platforms is now profoundly challenging\nThe post Stop Building AI Platforms appeared first on Towards Data Science.",
      "published_date": "2025-06-14T01:26:49+00:00",
      "collected_at": "2025-06-18T01:19:37.352611+00:00",
      "author": "Ming Gao",
      "source_priority": 3,
      "score": 90
    },
    {
      "id": "58f37280e0df15f573bc141bb368f7e0",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "What If I had AI in 2018: Rent the Runway Fulfillment Center Optimization",
      "url": "https://towardsdatascience.com/what-if-i-had-ai-in-2018-rent-the-runway-fulfillment-center-optimization/",
      "description": "An LLM in 2018 would not have trivialized a complex project, although it could have enhanced the final solution\nThe post What If I had AI in 2018: Rent the Runway Fulfillment Center Optimization appeared first on Towards Data Science.",
      "published_date": "2025-06-13T23:03:03+00:00",
      "collected_at": "2025-06-18T01:19:37.352803+00:00",
      "author": "Hugo Ducruc",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "ffdd109e3fa32ae7970118f5ab535914",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety",
      "url": "https://arxiv.org/abs/2506.12299",
      "description": "The recent advancements in Large Language Models(LLMs) have had a significant\nimpact on a wide range of fields, from general domains to specialized areas.\nHowever, these advancements have also significantly increased the potential for\nmalicious users to exploit harmful and jailbreak prompts for malicious attacks.\nAlthough there have been many efforts to prevent harmful prompts and jailbreak\nprompts, protecting LLMs from such malicious attacks remains an important and\nchallenging task. In this pa",
      "published_date": "2025-06-13T21:23:50+00:00",
      "collected_at": "2025-06-18T01:19:36.060076+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "08e4357cebd144a10ee62a390d9691a5",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "AI Is Not a Black Box (Relatively Speaking)",
      "url": "https://towardsdatascience.com/ai-is-not-a-black-box/",
      "description": "Compared to the opacity around human intelligence, AI is more transparent in some very tangible ways.\nThe post AI Is Not a Black Box (Relatively Speaking) appeared first on Towards Data Science.",
      "published_date": "2025-06-13T20:02:51+00:00",
      "collected_at": "2025-06-18T01:19:37.353005+00:00",
      "author": "Piotr (Peter) Mardziel",
      "source_priority": 3,
      "score": 90
    },
    {
      "id": "a47ee4e7d65a00ac2f3bffd11bc261a4",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "How AI Agents “Talk” to Each Other",
      "url": "https://towardsdatascience.com/how-ai-agents-talk-to-each-other/",
      "description": "Minimize chaos and maintain inter-agent harmony in your projects\nThe post How AI Agents “Talk” to Each Other appeared first on Towards Data Science.",
      "published_date": "2025-06-13T19:21:14+00:00",
      "collected_at": "2025-06-18T01:19:37.353212+00:00",
      "author": "TDS Editors",
      "source_priority": 3,
      "score": 90
    },
    {
      "id": "60c1fe34ea4d768996bae1b32a454c16",
      "source_id": "pytorch_blog",
      "source": "Blog – PyTorch",
      "category": "Open Source",
      "title": "ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization",
      "url": "https://pytorch.org/blog/paretoq-scaling-laws-in-extremely-low-bit-llm-quantization/",
      "description": "The field of large language models is shifting toward lower-precision computation. This shift necessitates a rethinking of scaling laws to account for the effects of quantization on resulting quantized model...",
      "published_date": "2025-06-13T18:43:38+00:00",
      "collected_at": "2025-06-18T01:19:36.249358+00:00",
      "author": "Zechun Liu, Changsheng Zhao, Hanxian Huang, Sijia Chen, Jing Zhang, Andrew Or, Jiawei Zhao, Scott Roy, Lisa Jin, Yunyang Xiong, Yangyang Shi, Lin Xiao, Yuandong Tian, Bilge Soran, Raghuraman Krishnamoorthi, Tijmen Blankevoort, Vikas Chandra (Meta)",
      "source_priority": 2,
      "score": 100
    },
    {
      "id": "990af43a8bd08aef867b008711a46162",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Hatevolution: What Static Benchmarks Don't Tell Us",
      "url": "https://arxiv.org/abs/2506.12148",
      "description": "Language changes over time, including in the hate speech domain, which\nevolves quickly following social dynamics and cultural shifts. While NLP\nresearch has investigated the impact of language evolution on model training\nand has proposed several solutions for it, its impact on model benchmarking\nremains under-explored. Yet, hate speech benchmarks play a crucial role to\nensure model safety. In this paper, we empirically evaluate the robustness of\n20 language models across two evolving hate speech",
      "published_date": "2025-06-13T14:08:19+00:00",
      "collected_at": "2025-06-18T01:19:36.058067+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "7571ffdddb4affdb11adf83f4a86ceda",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "VGR: Visual Grounded Reasoning",
      "url": "https://arxiv.org/abs/2506.11991",
      "description": "In the field of multimodal chain-of-thought (CoT) reasoning, existing\napproaches predominantly rely on reasoning on pure language space, which\ninherently suffers from language bias and is largely confined to math or\nscience domains. This narrow focus limits their ability to handle complex\nvisual reasoning tasks that demand comprehensive understanding of image\ndetails. To address these limitations, this paper introduces VGR, a novel\nreasoning multimodal large language model (MLLM) with enhanced f",
      "published_date": "2025-06-13T13:47:43+00:00",
      "collected_at": "2025-06-18T01:19:36.059300+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "2d83a9cb9a328057c618264033ae1dbe",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Connecting the Dots for Better Movie Recommendations",
      "url": "https://towardsdatascience.com/connecting-the-dots-for-better-movie-recommendations/",
      "description": "Connecting the Dots for Better Movie Recommendations: Lightweight graph RAG on Rotten Tomatoes movie reviews\nThe post Connecting the Dots for Better Movie Recommendations appeared first on Towards Data Science.",
      "published_date": "2025-06-13T00:27:55+00:00",
      "collected_at": "2025-06-18T01:19:37.353400+00:00",
      "author": "Brian Godsey",
      "source_priority": 3,
      "score": 89
    },
    {
      "id": "04bff2841ad31c6f572d7fe153ed41d4",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Agentic AI 103: Building Multi-Agent Teams",
      "url": "https://towardsdatascience.com/agentic-ai-103-building-multi-agent-teams/",
      "description": "Build multi-agent teams that can automate tasks and enhance productivity.\nThe post Agentic AI 103: Building Multi-Agent Teams appeared first on Towards Data Science.",
      "published_date": "2025-06-12T19:34:10+00:00",
      "collected_at": "2025-06-18T01:19:37.353586+00:00",
      "author": "Gustavo Santos",
      "source_priority": 3,
      "score": 88
    },
    {
      "id": "7a26e350ec7d0152163a44bb1175001a",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Design Smarter Prompts and Boost Your LLM Output: Real Tricks from an AI Engineer’s Toolbox",
      "url": "https://towardsdatascience.com/boost-your-llm-outputdesign-smarter-prompts-real-tricks-from-an-ai-engineers-toolbox/",
      "description": "Not just what you ask, but how you ask it. Practical techniques for prompt engineering that deliver\nThe post Design Smarter Prompts and Boost Your LLM Output: Real Tricks from an AI Engineer’s Toolbox appeared first on Towards Data Science.",
      "published_date": "2025-06-12T18:54:58+00:00",
      "collected_at": "2025-06-18T01:19:37.353779+00:00",
      "author": "Ugo Pradère",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "5d0a0ada767700d7cc88a28b0c2d8da5",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "User Authorisation in Streamlit With OIDC and Google",
      "url": "https://towardsdatascience.com/user-authorisation-in-streamlit-with-oidc-and-google/",
      "description": "Log in to a Streamlit app with a Google email account\nThe post User Authorisation in Streamlit With OIDC and Google appeared first on Towards Data Science.",
      "published_date": "2025-06-12T16:54:07+00:00",
      "collected_at": "2025-06-18T01:19:37.353968+00:00",
      "author": "Thomas Reid",
      "source_priority": 3,
      "score": 88
    },
    {
      "id": "c1685dfb6d0b3dcff3b54be485a863fb",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Exploring the Proportional Odds Model for Ordinal Logistic Regression",
      "url": "https://towardsdatascience.com/proportional-odds-model-for-ordinal-logistic-regression/",
      "description": "Understanding and Implementing Brant’s Tests in Ordinal Logistic Regression with Python\nThe post Exploring the Proportional Odds Model for Ordinal Logistic Regression appeared first on Towards Data Science.",
      "published_date": "2025-06-12T05:45:56+00:00",
      "collected_at": "2025-06-18T01:19:37.354309+00:00",
      "author": "JUNIOR JUMBONG",
      "source_priority": 3,
      "score": 88
    },
    {
      "id": "9063e838e64e6d653b3129520f884b40",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Can AI Truly Develop a Memory That Adapts Like Ours?",
      "url": "https://towardsdatascience.com/can-ai-truly-develop-a-memory-that-adapts-like-ours/",
      "description": "Exploring Titans: A new architecture equipping LLMs with human-inspired memory that learns and updates itself during test-time.\nThe post Can AI Truly Develop a Memory That Adapts Like Ours? appeared first on Towards Data Science.",
      "published_date": "2025-06-12T05:32:11+00:00",
      "collected_at": "2025-06-18T01:19:37.354501+00:00",
      "author": "Moulik Gupta",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "78f37795d518349beb00363896a87920",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Provably Learning from Language Feedback",
      "url": "https://arxiv.org/abs/2506.10341",
      "description": "Interactively learning from observation and language feedback is an\nincreasingly studied area driven by the emergence of large language model (LLM)\nagents. While impressive empirical demonstrations have been shown, so far a\nprincipled framing of these decision problems remains lacking. In this paper,\nwe formalize the Learning from Language Feedback (LLF) problem, assert\nsufficient assumptions to enable learning despite latent rewards, and introduce\ntransfer eluder dimension as a complexity measu",
      "published_date": "2025-06-12T00:35:02+00:00",
      "collected_at": "2025-06-18T01:19:36.056412+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "1c2ddf0dee0c54f5f6ff3fb07ff1b777",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification\n  and LLM Assistance",
      "url": "https://arxiv.org/abs/2506.09968",
      "description": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning.",
      "published_date": "2025-06-11T13:45:03+00:00",
      "collected_at": "2025-06-18T01:19:36.059543+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "6ff102bed1aaf30a009cca66babdcac5",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Marrying Autoregressive Transformer and Diffusion with Multi-Reference\n  Autoregression",
      "url": "https://arxiv.org/abs/2506.09482",
      "description": "We introduce TransDiff, the first image generation model that marries\nAutoregressive (AR) Transformer with diffusion models. In this joint modeling\nframework, TransDiff encodes labels and images into high-level semantic\nfeatures and employs a diffusion model to estimate the distribution of image\nsamples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms\nother image generation models based on standalone AR Transformer or diffusion\nmodels. Specifically, TransDiff achieves a Fr",
      "published_date": "2025-06-11T03:50:31+00:00",
      "collected_at": "2025-06-18T01:19:36.057259+00:00",
      "author": "",
      "source_priority": 1,
      "score": 98
    }
  ],
  "updated": "2025-06-18T01:20:17.149895"
}