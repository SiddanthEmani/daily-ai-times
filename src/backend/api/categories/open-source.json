{
  "category": "Open Source",
  "count": 37,
  "articles": [
    {
      "id": "32d314415eaab25648b9084330e0895e",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "A Practical Starters’ Guide to Causal Structure Learning with Bayesian Methods in Python",
      "url": "https://towardsdatascience.com/a-practical-starters-guide-to-causal-structure-learning-with-bayesian-methods-in-python/",
      "description": "Learn Causal Structures and make inferences with Bayesian Methods: Python Tutorial\nThe post A Practical Starters’ Guide to Causal Structure Learning with Bayesian Methods in Python appeared first on Towards Data Science.",
      "published_date": "2025-06-17T00:45:46+00:00",
      "collected_at": "2025-06-17T04:26:06.981785+00:00",
      "author": "Erdogan Taskesen",
      "source_priority": 3,
      "score": 95
    },
    {
      "id": "3b18452ba89445f24ebc15f969f67868",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Let’s Analyze OpenAI’s Claims About ChatGPT Energy Use",
      "url": "https://towardsdatascience.com/lets-analyze-openais-claims-about-chatgpt-energy-use/",
      "description": "ChatGPT uses an average of 0.34 Wh per query, according to a blog post by Sam Altman. Does that figure hold up?\nThe post Let’s Analyze OpenAI’s Claims About ChatGPT Energy Use appeared first on Towards Data Science.",
      "published_date": "2025-06-16T19:31:58+00:00",
      "collected_at": "2025-06-17T04:26:06.982000+00:00",
      "author": "Kasper Groes Albin Ludvigsen",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "8d8888a67d52c0320ffd86cbc05ed4cb",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Regularisation: A Deep Dive into Theory, Implementation, and Practical Insights",
      "url": "https://towardsdatascience.com/regularisation-a-deep-dive-into-theory-implementation-and-practical-insights/",
      "description": "A detailed guide on controlling overfitting and increasing the stability of your models.\nThe post Regularisation: A Deep Dive into Theory, Implementation, and Practical Insights appeared first on Towards Data Science.",
      "published_date": "2025-06-16T19:16:38+00:00",
      "collected_at": "2025-06-17T04:26:06.982227+00:00",
      "author": "Sourav Mohile",
      "source_priority": 3,
      "score": 95
    },
    {
      "id": "a0092c46510f51961c64ac45063c699d",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Build an AI Agent to Explore Your Data Catalog with Natural Language",
      "url": "https://towardsdatascience.com/build-and-ai-agent-to-explore-your-data-catalog-with-natural-language/",
      "description": "Leverage LLMs to query your Databricks Data Catalog\nThe post Build an AI Agent to Explore Your Data Catalog with Natural Language appeared first on Towards Data Science.",
      "published_date": "2025-06-16T17:37:42+00:00",
      "collected_at": "2025-06-17T04:26:06.982426+00:00",
      "author": "Fabiana Clemente",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "38317db47248e6c73c6016a6ca7a648a",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "I Won $10,000 in a Machine Learning Competition — Here’s My Complete Strategy",
      "url": "https://towardsdatascience.com/i-won-10000-in-a-machine-learning-competition-heres-my-complete-strategy/",
      "description": "Complete guide to feature selection, threshold optimization, and neural network architecture for ML competitions\nThe post I Won $10,000 in a Machine Learning Competition — Here’s My Complete Strategy appeared first on Towards Data Science.",
      "published_date": "2025-06-16T17:12:24+00:00",
      "collected_at": "2025-06-17T04:26:06.982624+00:00",
      "author": "Claudia Ng",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "c3ad6fbaec8be7e9f6249a3c8589b100",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Agents, APIs, and the Next Layer of the Internet",
      "url": "https://towardsdatascience.com/agents-apis-and-the-next-layer-of-the-internet/",
      "description": "Part I: Shipping Containers for Thought Every so often a simple idea rewires everything. The shipping container didn’t just optimise logistics; it flattened the globe, collapsed time zones, and rewrote the economics of trade. In its geometric austerity was a quiet revolution: standardisation. Similarly, HTML and HTTP didn’t invent information exchange — any more than […]\nThe post Agents, APIs, and the Next Layer of the Internet appeared first on Towards Data Science.",
      "published_date": "2025-06-16T17:06:00+00:00",
      "collected_at": "2025-06-17T04:26:06.982837+00:00",
      "author": "Cooper Doyle",
      "source_priority": 3,
      "score": 95
    },
    {
      "id": "e507af3454542f87043c1f468a456f33",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey",
      "url": "https://arxiv.org/abs/2506.13759",
      "description": "In this work, we provide a systematic survey of Discrete Diffusion Language\nModels (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).\nUnlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,\nparallel decoding paradigm using full attention and a denoising-based\ngeneration strategy. This paradigm naturally enables parallel generation,\nfine-grained output controllability, and dynamic, response-aware perception.\nThese capabilities are previously difficult to achieve",
      "published_date": "2025-06-16T13:59:08+00:00",
      "collected_at": "2025-06-17T04:26:05.407106+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "1300e898ebc5f3d688f89ee0db7e08cb",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Steering LLM Thinking with Budget Guidance",
      "url": "https://arxiv.org/abs/2506.13752",
      "description": "Recent deep-thinking large language models often reason extensively to\nimprove performance, but such lengthy reasoning is not always desirable, as it\nincurs excessive inference costs with disproportionate performance gains.\nControlling reasoning length without sacrificing performance is therefore\nimportant, but remains challenging, especially under tight thinking budgets. We\npropose budget guidance, a simple yet effective method for steering the\nreasoning process of LLMs toward a target budget w",
      "published_date": "2025-06-16T13:57:05+00:00",
      "collected_at": "2025-06-17T04:26:05.405636+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "995709b6f4863f677260df34fd04b10a",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Test3R: Learning to Reconstruct 3D at Test Time",
      "url": "https://arxiv.org/abs/2506.13750",
      "description": "Dense matching methods like DUSt3R regress pairwise pointmaps for 3D\nreconstruction. However, the reliance on pairwise prediction and the limited\ngeneralization capability inherently restrict the global geometric consistency.\nIn this work, we introduce Test3R, a surprisingly simple test-time learning\ntechnique that significantly boosts geometric accuracy. Using image triplets\n(I_1,I_2,I_3), Test3R generates reconstructions from pairs (I_1,I_2) and\n(I_1,I_3). The core idea is to optimize the netw",
      "published_date": "2025-06-16T13:56:22+00:00",
      "collected_at": "2025-06-17T04:26:05.406862+00:00",
      "author": "",
      "source_priority": 1,
      "score": 96
    },
    {
      "id": "2d461f36935ee4eb3c7a6fe97be75e35",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning",
      "url": "https://arxiv.org/abs/2506.13654",
      "description": "We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e.,\nin days and weeks) egocentric videos, which leverages a structured\nChain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained\nvia reinforcement learning (RL). Inspired by human problem-solving strategies,\nCoTT decomposes complex reasoning into modular steps, with the RL agent\ninvoking specific tools, one per step, to iteratively and collaboratively\nanswer sub-questions tackling such tasks as temporal r",
      "published_date": "2025-06-16T12:17:08+00:00",
      "collected_at": "2025-06-17T04:26:05.405128+00:00",
      "author": "",
      "source_priority": 1,
      "score": 95
    },
    {
      "id": "62a3dbbebf63aba9082835010414aa83",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning\n  Attention",
      "url": "https://arxiv.org/abs/2506.13585",
      "description": "We introduce MiniMax-M1, the world's first open-weight, large-scale\nhybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid\nMixture-of-Experts (MoE) architecture combined with a lightning attention\nmechanism. The model is developed based on our previous MiniMax-Text-01 model,\nwhich contains a total of 456 billion parameters with 45.9 billion parameters\nactivated per token. The M1 model natively supports a context length of 1\nmillion tokens, 8x the context size of DeepSeek R1. Further",
      "published_date": "2025-06-16T11:08:02+00:00",
      "collected_at": "2025-06-17T04:26:05.407325+00:00",
      "author": "",
      "source_priority": 1,
      "score": 95
    },
    {
      "id": "c537aa8dc41a5e0c7806b9cda18a00f2",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Forecasting Time Series with LLMs via Patch-Based Prompting and\n  Decomposition",
      "url": "https://arxiv.org/abs/2506.12953",
      "description": "Recent advances in Large Language Models (LLMs) have demonstrated new\npossibilities for accurate and efficient time series analysis, but prior work\noften required heavy fine-tuning and/or ignored inter-series correlations. In\nthis work, we explore simple and flexible prompt-based strategies that enable\nLLMs to perform time series forecasting without extensive retraining or the use\nof a complex external architecture. Through the exploration of specialized\nprompting methods that leverage time seri",
      "published_date": "2025-06-15T15:42:58+00:00",
      "collected_at": "2025-06-17T04:26:05.406588+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "f91f23c6337c4cad91dedf70a6357a67",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "PersonaFeedback: A Large-scale Human-annotated Benchmark For\n  Personalization",
      "url": "https://arxiv.org/abs/2506.12915",
      "description": "With the rapid improvement in the general capabilities of LLMs, LLM\npersonalization, i.e., how to build LLM systems that can generate personalized\nresponses or services that are tailored to distinct user personas, has become\nan increasingly important research and engineering problem. However, unlike\nmany new challenging benchmarks being released for evaluating the\ngeneral/reasoning capabilities, the lack of high-quality benchmarks for\nevaluating LLM personalization greatly hinders progress in th",
      "published_date": "2025-06-15T13:19:19+00:00",
      "collected_at": "2025-06-17T04:26:05.406122+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "5daa16b2d3ef97351631e499c716819a",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "MS4UI: A Dataset for Multi-modal Summarization of User Interface\n  Instructional Videos",
      "url": "https://arxiv.org/abs/2506.12623",
      "description": "We study multi-modal summarization for instructional videos, whose goal is to\nprovide users an efficient way to learn skills in the form of text instructions\nand key video frames. We observe that existing benchmarks focus on generic\nsemantic-level video summarization, and are not suitable for providing\nstep-by-step executable instructions and illustrations, both of which are\ncrucial for instructional videos. We propose a novel benchmark for user\ninterface (UI) instructional video summarization t",
      "published_date": "2025-06-14T16:39:32+00:00",
      "collected_at": "2025-06-17T04:26:05.406349+00:00",
      "author": "",
      "source_priority": 1,
      "score": 93
    },
    {
      "id": "945040590a52d22cce6ac17701f23e7f",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Language Surgery in Multilingual Large Language Models",
      "url": "https://arxiv.org/abs/2506.12450",
      "description": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across tasks and languages, revolutionizing natural language\nprocessing. This paper investigates the naturally emerging representation\nalignment in LLMs, particularly in the middle layers, and its implications for\ndisentangling language-specific and language-agnostic information. We\nempirically confirm the existence of this alignment, analyze its behavior in\ncomparison to explicitly designed alignment models, a",
      "published_date": "2025-06-14T07:09:50+00:00",
      "collected_at": "2025-06-17T04:26:05.405866+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "0982c8bdf5f0c9191bd5ac8f4ecf7bf5",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Stop Building AI Platforms",
      "url": "https://towardsdatascience.com/stop-building-ai-platforms/",
      "description": "When small and medium companies achieve success in building Data and ML platforms, building AI platforms is now profoundly challenging\nThe post Stop Building AI Platforms appeared first on Towards Data Science.",
      "published_date": "2025-06-14T01:26:49+00:00",
      "collected_at": "2025-06-17T04:26:06.983034+00:00",
      "author": "Ming Gao",
      "source_priority": 3,
      "score": 91
    },
    {
      "id": "58f37280e0df15f573bc141bb368f7e0",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "What If I had AI in 2018: Rent the Runway Fulfillment Center Optimization",
      "url": "https://towardsdatascience.com/what-if-i-had-ai-in-2018-rent-the-runway-fulfillment-center-optimization/",
      "description": "An LLM in 2018 would not have trivialized a complex project, although it could have enhanced the final solution\nThe post What If I had AI in 2018: Rent the Runway Fulfillment Center Optimization appeared first on Towards Data Science.",
      "published_date": "2025-06-13T23:03:03+00:00",
      "collected_at": "2025-06-17T04:26:06.983251+00:00",
      "author": "Hugo Ducruc",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "08e4357cebd144a10ee62a390d9691a5",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "AI Is Not a Black Box (Relatively Speaking)",
      "url": "https://towardsdatascience.com/ai-is-not-a-black-box/",
      "description": "Compared to the opacity around human intelligence, AI is more transparent in some very tangible ways.\nThe post AI Is Not a Black Box (Relatively Speaking) appeared first on Towards Data Science.",
      "published_date": "2025-06-13T20:02:51+00:00",
      "collected_at": "2025-06-17T04:26:06.983444+00:00",
      "author": "Piotr (Peter) Mardziel",
      "source_priority": 3,
      "score": 91
    },
    {
      "id": "a47ee4e7d65a00ac2f3bffd11bc261a4",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "How AI Agents “Talk” to Each Other",
      "url": "https://towardsdatascience.com/how-ai-agents-talk-to-each-other/",
      "description": "Minimize chaos and maintain inter-agent harmony in your projects\nThe post How AI Agents “Talk” to Each Other appeared first on Towards Data Science.",
      "published_date": "2025-06-13T19:21:14+00:00",
      "collected_at": "2025-06-17T04:26:06.983646+00:00",
      "author": "TDS Editors",
      "source_priority": 3,
      "score": 91
    },
    {
      "id": "60c1fe34ea4d768996bae1b32a454c16",
      "source_id": "pytorch_blog",
      "source": "Blog – PyTorch",
      "category": "Open Source",
      "title": "ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization",
      "url": "https://pytorch.org/blog/paretoq-scaling-laws-in-extremely-low-bit-llm-quantization/",
      "description": "The field of large language models is shifting toward lower-precision computation. This shift necessitates a rethinking of scaling laws to account for the effects of quantization on resulting quantized model...",
      "published_date": "2025-06-13T18:43:38+00:00",
      "collected_at": "2025-06-17T04:26:05.860982+00:00",
      "author": "Zechun Liu, Changsheng Zhao, Hanxian Huang, Sijia Chen, Jing Zhang, Andrew Or, Jiawei Zhao, Scott Roy, Lisa Jin, Yunyang Xiong, Yangyang Shi, Lin Xiao, Yuandong Tian, Bilge Soran, Raghuraman Krishnamoorthi, Tijmen Blankevoort, Vikas Chandra (Meta)",
      "source_priority": 2,
      "score": 100
    },
    {
      "id": "d5ee284a28504a72e1e5d37392fd5493",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Supernova Event Dataset: Interpreting Large Language Model's Personality\n  through Critical Event Analysis",
      "url": "https://arxiv.org/abs/2506.12189",
      "description": "Large Language Models (LLMs) are increasingly integrated into everyday\napplications. As their influence grows, understanding their decision making and\nunderlying personality becomes essential. In this work, we interpret model\npersonality using our proposed Supernova Event Dataset, a novel dataset with\ndiverse articles spanning biographies, historical events, news, and scientific\ndiscoveries. We use this dataset to benchmark LLMs on extracting and ranking\nkey events from text, a subjective and co",
      "published_date": "2025-06-13T15:31:52+00:00",
      "collected_at": "2025-06-17T04:26:05.407781+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "53f63f8182a6a0cad04b4af732569a66",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents",
      "url": "https://arxiv.org/abs/2506.11763",
      "description": "Deep Research Agents are a prominent category of LLM-based agents. By\nautonomously orchestrating multistep web exploration, targeted retrieval, and\nhigher-order synthesis, they transform vast amounts of online information into\nanalyst-grade, citation-rich reports--compressing hours of manual desk research\ninto minutes. However, a comprehensive benchmark for systematically evaluating\nthe capabilities of these agents remains absent. To bridge this gap, we present\nDeepResearch Bench, a benchmark co",
      "published_date": "2025-06-13T09:17:32+00:00",
      "collected_at": "2025-06-17T04:26:05.408478+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "2d83a9cb9a328057c618264033ae1dbe",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Connecting the Dots for Better Movie Recommendations",
      "url": "https://towardsdatascience.com/connecting-the-dots-for-better-movie-recommendations/",
      "description": "Connecting the Dots for Better Movie Recommendations: Lightweight graph RAG on Rotten Tomatoes movie reviews\nThe post Connecting the Dots for Better Movie Recommendations appeared first on Towards Data Science.",
      "published_date": "2025-06-13T00:27:55+00:00",
      "collected_at": "2025-06-17T04:26:06.983834+00:00",
      "author": "Brian Godsey",
      "source_priority": 3,
      "score": 90
    },
    {
      "id": "04bff2841ad31c6f572d7fe153ed41d4",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Agentic AI 103: Building Multi-Agent Teams",
      "url": "https://towardsdatascience.com/agentic-ai-103-building-multi-agent-teams/",
      "description": "Build multi-agent teams that can automate tasks and enhance productivity.\nThe post Agentic AI 103: Building Multi-Agent Teams appeared first on Towards Data Science.",
      "published_date": "2025-06-12T19:34:10+00:00",
      "collected_at": "2025-06-17T04:26:06.984023+00:00",
      "author": "Gustavo Santos",
      "source_priority": 3,
      "score": 89
    },
    {
      "id": "7a26e350ec7d0152163a44bb1175001a",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Design Smarter Prompts and Boost Your LLM Output: Real Tricks from an AI Engineer’s Toolbox",
      "url": "https://towardsdatascience.com/boost-your-llm-outputdesign-smarter-prompts-real-tricks-from-an-ai-engineers-toolbox/",
      "description": "Not just what you ask, but how you ask it. Practical techniques for prompt engineering that deliver\nThe post Design Smarter Prompts and Boost Your LLM Output: Real Tricks from an AI Engineer’s Toolbox appeared first on Towards Data Science.",
      "published_date": "2025-06-12T18:54:58+00:00",
      "collected_at": "2025-06-17T04:26:06.984238+00:00",
      "author": "Ugo Pradère",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "2e8809fe54b65a3d0d5fd121476d478e",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Don't Pay Attention",
      "url": "https://arxiv.org/abs/2506.11305",
      "description": "The Transformer has become the de facto standard for large language models\nand a wide range of downstream tasks across various domains. Despite its\nnumerous advantages like inherent training parallelism, the Transformer still\nfaces key challenges due to its inability to effectively process sequences\nbeyond a fixed context window and the quadratic complexity of its attention\nmechanism. These challenges have renewed interest in RNN-like architectures,\nwhich offer linear scaling with sequence lengt",
      "published_date": "2025-06-12T17:11:06+00:00",
      "collected_at": "2025-06-17T04:26:05.408690+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "5d0a0ada767700d7cc88a28b0c2d8da5",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "User Authorisation in Streamlit With OIDC and Google",
      "url": "https://towardsdatascience.com/user-authorisation-in-streamlit-with-oidc-and-google/",
      "description": "Log in to a Streamlit app with a Google email account\nThe post User Authorisation in Streamlit With OIDC and Google appeared first on Towards Data Science.",
      "published_date": "2025-06-12T16:54:07+00:00",
      "collected_at": "2025-06-17T04:26:06.984431+00:00",
      "author": "Thomas Reid",
      "source_priority": 3,
      "score": 89
    },
    {
      "id": "c1685dfb6d0b3dcff3b54be485a863fb",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Exploring the Proportional Odds Model for Ordinal Logistic Regression",
      "url": "https://towardsdatascience.com/proportional-odds-model-for-ordinal-logistic-regression/",
      "description": "Understanding and Implementing Brant’s Tests in Ordinal Logistic Regression with Python\nThe post Exploring the Proportional Odds Model for Ordinal Logistic Regression appeared first on Towards Data Science.",
      "published_date": "2025-06-12T05:45:56+00:00",
      "collected_at": "2025-06-17T04:26:06.984628+00:00",
      "author": "JUNIOR JUMBONG",
      "source_priority": 3,
      "score": 89
    },
    {
      "id": "9063e838e64e6d653b3129520f884b40",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Can AI Truly Develop a Memory That Adapts Like Ours?",
      "url": "https://towardsdatascience.com/can-ai-truly-develop-a-memory-that-adapts-like-ours/",
      "description": "Exploring Titans: A new architecture equipping LLMs with human-inspired memory that learns and updates itself during test-time.\nThe post Can AI Truly Develop a Memory That Adapts Like Ours? appeared first on Towards Data Science.",
      "published_date": "2025-06-12T05:32:11+00:00",
      "collected_at": "2025-06-17T04:26:06.984821+00:00",
      "author": "Moulik Gupta",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "1d4fb1472cacc35edd4301034d5dc3c9",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via\n  Perception, Understanding, and Reasoning",
      "url": "https://arxiv.org/abs/2506.10521",
      "description": "Scientific discoveries increasingly rely on complex multimodal reasoning\nbased on information-intensive scientific data and domain-specific expertise.\nEmpowered by expert-level scientific benchmarks, scientific Multimodal Large\nLanguage Models (MLLMs) hold the potential to significantly enhance this\ndiscovery process in realistic workflows. However, current scientific\nbenchmarks mostly focus on evaluating the knowledge understanding capabilities\nof MLLMs, leading to an inadequate assessment of t",
      "published_date": "2025-06-12T05:29:16+00:00",
      "collected_at": "2025-06-17T04:26:05.405387+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "c61c90cc051615c1d6d0c30f5fda157d",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal\n  Skills",
      "url": "https://arxiv.org/abs/2506.10387",
      "description": "Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI\nagents have yielded promising outcomes. However, these agents still struggle\nwith long-horizon tasks in online environments, primarily due to insufficient\nknowledge and the inherent gap between offline and online domains. In this\npaper, inspired by how humans generalize knowledge in open-ended environments,\nwe propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of\ninsufficient knowledge. It progre",
      "published_date": "2025-06-12T02:21:19+00:00",
      "collected_at": "2025-06-17T04:26:05.409348+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "f6d81dda61f177f4ac4a259da522fa1d",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Model Context Protocol (MCP) Tutorial: Build Your First MCP Server in 6 Steps",
      "url": "https://towardsdatascience.com/model-context-protocol-mcp-tutorial-build-your-first-mcp-server-in-6-steps/",
      "description": "A beginner-friendly tutorial of MCP architecture, with the focus on MCP server components and applications, guiding through the process of building a custom MCP server that enables code-to-diagram.\nThe post Model Context Protocol (MCP) Tutorial: Build Your First MCP Server in 6 Steps appeared first on Towards Data Science.",
      "published_date": "2025-06-11T19:40:45+00:00",
      "collected_at": "2025-06-17T04:26:06.985012+00:00",
      "author": "Destin Gong",
      "source_priority": 3,
      "score": 88
    },
    {
      "id": "036d3d830f2eae87fdae67f04880d8b8",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Mobile App Development with Python",
      "url": "https://towardsdatascience.com/mobile-app-development-with-python/",
      "description": "Build iOS & Android Apps with Kivy\nThe post Mobile App Development with Python appeared first on Towards Data Science.",
      "published_date": "2025-06-11T16:55:03+00:00",
      "collected_at": "2025-06-17T04:26:07.009931+00:00",
      "author": "Mauro Di Pietro",
      "source_priority": 3,
      "score": 88
    },
    {
      "id": "7cee2b0c3b922141596bbb4f9d9c4972",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Reward Models Enable Scalable Code Verification by Trading Accuracy for\n  Throughput",
      "url": "https://arxiv.org/abs/2506.10056",
      "description": "The standard paradigm for solving coding tasks via large language models\n(LLMs) is to generate-then-rank programs, where the latter step uses a verifier\nin the ranking process. The growing consensus is that a comprehensive verifier\n(e.g., a full test suite) should be prioritized over an outcome reward model\n(ORM) whenever possible, with little consideration given to the trade-offs\ninvolved. We aim to challenge this assumption by systematically exploring the\ntradeoff between speed and accuracy. W",
      "published_date": "2025-06-11T13:58:21+00:00",
      "collected_at": "2025-06-17T04:26:05.409134+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "8647cd9cdb434e35eec9fb9d5d10b3c7",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Audio Spectrogram Transformers Beyond the Lab",
      "url": "https://towardsdatascience.com/audio-spectrogram-transformers-beyond-the-lab/",
      "description": "A recipe for building a portable soundscape monitoring app with AudioMoth, Raspberry Pi, and a decent dose of deep learning.\nThe post Audio Spectrogram Transformers Beyond the Lab appeared first on Towards Data Science.",
      "published_date": "2025-06-10T23:47:29+00:00",
      "collected_at": "2025-06-17T04:26:07.010195+00:00",
      "author": "Maciej Adamiak",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "670434083c4a15bec64ebb9e2cdcc44c",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Automate Models Training: An MLOps Pipeline with Tekton and Buildpacks",
      "url": "https://towardsdatascience.com/automate-models-training-an-mlops-pipeline-with-tekton-and-buildpacks/",
      "description": "A step-by-step guide to containerizing and orchestrating an ML training workflow without the Dockerfile headache, using a lightweight GPT-2 example.\nThe post Automate Models Training: An MLOps Pipeline with Tekton and Buildpacks appeared first on Towards Data Science.",
      "published_date": "2025-06-10T23:37:18+00:00",
      "collected_at": "2025-06-17T04:26:07.010404+00:00",
      "author": "Sylvain Kalache",
      "source_priority": 3,
      "score": 97
    },
    {
      "id": "cbcc5d62f662fae27950ed1aa42f2e83",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions",
      "url": "https://arxiv.org/abs/2506.09038",
      "description": "For Large Language Models (LLMs) to be reliably deployed in both everyday and\nhigh-stakes domains, knowing when not to answer is equally critical as\nanswering correctly. Real-world user queries, which can be underspecified,\nill-posed, or fundamentally unanswerable, require LLMs to reason about\nuncertainty and selectively abstain -- i.e., refuse to answer definitively.\nHowever, abstention remains understudied, without a systematic evaluation\nframework for modern LLMs. In this work, we introduce A",
      "published_date": "2025-06-10T13:57:30+00:00",
      "collected_at": "2025-06-17T04:26:05.408902+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    }
  ],
  "updated": "2025-06-17T04:26:59.161886"
}