{
  "category": "Open Source",
  "count": 41,
  "articles": [
    {
      "id": "2d83a9cb9a328057c618264033ae1dbe",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Connecting the Dots for Better Movie Recommendations",
      "url": "https://towardsdatascience.com/connecting-the-dots-for-better-movie-recommendations/",
      "description": "Connecting the Dots for Better Movie Recommendations: Lightweight graph RAG on Rotten Tomatoes movie reviews\nThe post Connecting the Dots for Better Movie Recommendations appeared first on Towards Data Science.",
      "published_date": "2025-06-13T00:27:55+00:00",
      "collected_at": "2025-06-13T05:46:50.693758+00:00",
      "author": "Brian Godsey",
      "source_priority": 3,
      "score": 95
    },
    {
      "id": "04bff2841ad31c6f572d7fe153ed41d4",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Agentic AI 103: Building Multi-Agent Teams",
      "url": "https://towardsdatascience.com/agentic-ai-103-building-multi-agent-teams/",
      "description": "Build multi-agent teams that can automate tasks and enhance productivity.\nThe post Agentic AI 103: Building Multi-Agent Teams appeared first on Towards Data Science.",
      "published_date": "2025-06-12T19:34:10+00:00",
      "collected_at": "2025-06-13T05:46:50.693967+00:00",
      "author": "Gustavo Santos",
      "source_priority": 3,
      "score": 95
    },
    {
      "id": "7a26e350ec7d0152163a44bb1175001a",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Design Smarter Prompts and Boost Your LLM Output: Real Tricks from an AI Engineer’s Toolbox",
      "url": "https://towardsdatascience.com/boost-your-llm-outputdesign-smarter-prompts-real-tricks-from-an-ai-engineers-toolbox/",
      "description": "Not just what you ask, but how you ask it. Practical techniques for prompt engineering that deliver\nThe post Design Smarter Prompts and Boost Your LLM Output: Real Tricks from an AI Engineer’s Toolbox appeared first on Towards Data Science.",
      "published_date": "2025-06-12T18:54:58+00:00",
      "collected_at": "2025-06-13T05:46:50.694179+00:00",
      "author": "Ugo Pradère",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "5d0a0ada767700d7cc88a28b0c2d8da5",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "User Authorisation in Streamlit With OIDC and Google",
      "url": "https://towardsdatascience.com/user-authorisation-in-streamlit-with-oidc-and-google/",
      "description": "Log in to a Streamlit app with a Google email account\nThe post User Authorisation in Streamlit With OIDC and Google appeared first on Towards Data Science.",
      "published_date": "2025-06-12T16:54:07+00:00",
      "collected_at": "2025-06-13T05:46:50.694404+00:00",
      "author": "Thomas Reid",
      "source_priority": 3,
      "score": 95
    },
    {
      "id": "936f31fc98bda2bcd4d5f358e69af710",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
      "url": "https://arxiv.org/abs/2506.10974",
      "description": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex,",
      "published_date": "2025-06-12T13:59:32+00:00",
      "collected_at": "2025-06-13T05:46:49.351955+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "5bbff26f9ec82300f61da365ff122f59",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark",
      "url": "https://arxiv.org/abs/2506.10960",
      "description": "Large language models (LLMs) have been increasingly applied to automated\nharmful content detection tasks, assisting moderators in identifying policy\nviolations and improving the overall efficiency and accuracy of content review.\nHowever, existing resources for harmful content detection are predominantly\nfocused on English, with Chinese datasets remaining scarce and often limited in\nscope. We present a comprehensive, professionally annotated benchmark for\nChinese content harm detection, which cov",
      "published_date": "2025-06-12T13:57:05+00:00",
      "collected_at": "2025-06-13T05:46:49.352256+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "7a8f9ad549edffa35b8df98f49986c71",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Build the web for agents, not agents for the web",
      "url": "https://arxiv.org/abs/2506.10953",
      "description": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of",
      "published_date": "2025-06-12T13:53:58+00:00",
      "collected_at": "2025-06-13T05:46:49.353923+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "79eca08b5853abc5578275b822338497",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture\n  without Training",
      "url": "https://arxiv.org/abs/2506.10952",
      "description": "We introduce~Domain2Vec, a novel approach that decomposes any\ndataset into a linear combination of several meta-domains, a new concept\ndesigned to capture the key underlying features of datasets.\nDomain2Vec maintains a vocabulary of meta-domains and uses a\nclassifier to decompose any given dataset into a domain vector that corresponds\nto a distribution over this vocabulary. These domain vectors enable the\nidentification of the optimal data mixture for language model (LM) pretraining\nin a trainin",
      "published_date": "2025-06-12T13:53:51+00:00",
      "collected_at": "2025-06-13T05:46:49.354431+00:00",
      "author": "",
      "source_priority": 1,
      "score": 95
    },
    {
      "id": "19a6843226167888e5296d134d798fd3",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic\n  Design Generation",
      "url": "https://arxiv.org/abs/2506.10890",
      "description": "Graphic design plays a crucial role in both commercial and personal contexts,\nyet creating high-quality, editable, and aesthetically pleasing graphic\ncompositions remains a time-consuming and skill-intensive task, especially for\nbeginners. Current AI tools automate parts of the workflow, but struggle to\naccurately incorporate user-supplied assets, maintain editability, and achieve\nprofessional visual appeal. Commercial systems, like Canva Magic Design, rely\non vast template libraries, which are",
      "published_date": "2025-06-12T12:54:39+00:00",
      "collected_at": "2025-06-13T05:46:49.355222+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "454a86129b2972f186e544325d05a139",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "VideoDeepResearch: Long Video Understanding With Agentic Tool Using",
      "url": "https://arxiv.org/abs/2506.10821",
      "description": "Long video understanding (LVU) presents a significant challenge for current\nmulti-modal large language models (MLLMs) due to the task's inherent complexity\nand context window constraint. It is widely assumed that addressing LVU tasks\nrequires foundation MLLMs with extended context windows, strong visual\nperception capabilities, and proficient domain expertise. In this work, we\nchallenge this common belief by introducing VideoDeepResearch, a novel agentic\nframework for long video understanding. O",
      "published_date": "2025-06-12T11:39:10+00:00",
      "collected_at": "2025-06-13T05:46:49.354667+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "4c6f175ac4fdbeb60ce5b925c15d0ca6",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation",
      "url": "https://arxiv.org/abs/2506.10540",
      "description": "Despite rapid advancements in video generation models, generating coherent\nstorytelling videos that span multiple scenes and characters remains\nchallenging. Current methods often rigidly convert pre-generated keyframes into\nfixed-length clips, resulting in disjointed narratives and pacing issues.\nFurthermore, the inherent instability of video generation models means that\neven a single low-quality clip can significantly degrade the entire output\nanimation's logical coherence and visual continuity",
      "published_date": "2025-06-12T06:06:21+00:00",
      "collected_at": "2025-06-13T05:46:49.352855+00:00",
      "author": "",
      "source_priority": 1,
      "score": 95
    },
    {
      "id": "c1685dfb6d0b3dcff3b54be485a863fb",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Exploring the Proportional Odds Model for Ordinal Logistic Regression",
      "url": "https://towardsdatascience.com/proportional-odds-model-for-ordinal-logistic-regression/",
      "description": "Understanding and Implementing Brant’s Tests in Ordinal Logistic Regression with Python\nThe post Exploring the Proportional Odds Model for Ordinal Logistic Regression appeared first on Towards Data Science.",
      "published_date": "2025-06-12T05:45:56+00:00",
      "collected_at": "2025-06-13T05:46:50.694609+00:00",
      "author": "JUNIOR JUMBONG",
      "source_priority": 3,
      "score": 94
    },
    {
      "id": "9063e838e64e6d653b3129520f884b40",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Can AI Truly Develop a Memory That Adapts Like Ours?",
      "url": "https://towardsdatascience.com/can-ai-truly-develop-a-memory-that-adapts-like-ours/",
      "description": "Exploring Titans: A new architecture equipping LLMs with human-inspired memory that learns and updates itself during test-time.\nThe post Can AI Truly Develop a Memory That Adapts Like Ours? appeared first on Towards Data Science.",
      "published_date": "2025-06-12T05:32:11+00:00",
      "collected_at": "2025-06-13T05:46:50.694807+00:00",
      "author": "Moulik Gupta",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "b60c248f372be8d09338700bd0b0e697",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Discovering Hierarchical Latent Capabilities of Language Models via\n  Causal Representation Learning",
      "url": "https://arxiv.org/abs/2506.10378",
      "description": "Faithful evaluation of language model capabilities is crucial for deriving\nactionable insights that can inform model development. However, rigorous causal\nevaluations in this domain face significant methodological challenges,\nincluding complex confounding effects and prohibitive computational costs\nassociated with extensive retraining. To tackle these challenges, we propose a\ncausal representation learning framework wherein observed benchmark performance\nis modeled as a linear transformation of",
      "published_date": "2025-06-12T02:07:42+00:00",
      "collected_at": "2025-06-13T05:46:49.352556+00:00",
      "author": "",
      "source_priority": 1,
      "score": 95
    },
    {
      "id": "d39b518f9228857be99f8e92d1c2534f",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable\n  Task Experts",
      "url": "https://arxiv.org/abs/2506.10357",
      "description": "Recently, agents based on multimodal large language models (MLLMs) have\nachieved remarkable progress across various domains. However, building a\ngeneralist agent with capabilities such as perception, planning, action,\ngrounding, and reflection in open-world environments like Minecraft remains\nchallenges: insufficient domain-specific data, interference among heterogeneous\ntasks, and visual diversity in open-world settings. In this paper, we address\nthese challenges through three key contributions",
      "published_date": "2025-06-12T01:29:40+00:00",
      "collected_at": "2025-06-13T05:46:49.355715+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "f6d81dda61f177f4ac4a259da522fa1d",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Model Context Protocol (MCP) Tutorial: Build Your First MCP Server in 6 Steps",
      "url": "https://towardsdatascience.com/model-context-protocol-mcp-tutorial-build-your-first-mcp-server-in-6-steps/",
      "description": "A beginner-friendly tutorial of MCP architecture, with the focus on MCP server components and applications, guiding through the process of building a custom MCP server that enables code-to-diagram.\nThe post Model Context Protocol (MCP) Tutorial: Build Your First MCP Server in 6 Steps appeared first on Towards Data Science.",
      "published_date": "2025-06-11T19:40:45+00:00",
      "collected_at": "2025-06-13T05:46:50.695003+00:00",
      "author": "Destin Gong",
      "source_priority": 3,
      "score": 93
    },
    {
      "id": "036d3d830f2eae87fdae67f04880d8b8",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Mobile App Development with Python",
      "url": "https://towardsdatascience.com/mobile-app-development-with-python/",
      "description": "Build iOS & Android Apps with Kivy\nThe post Mobile App Development with Python appeared first on Towards Data Science.",
      "published_date": "2025-06-11T16:55:03+00:00",
      "collected_at": "2025-06-13T05:46:50.707121+00:00",
      "author": "Mauro Di Pietro",
      "source_priority": 3,
      "score": 93
    },
    {
      "id": "89fa0d60893cd64ee595dd2987d29c2b",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Text-Aware Image Restoration with Diffusion Models",
      "url": "https://arxiv.org/abs/2506.09993",
      "description": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of",
      "published_date": "2025-06-11T13:59:46+00:00",
      "collected_at": "2025-06-13T05:46:49.356428+00:00",
      "author": "",
      "source_priority": 1,
      "score": 94
    },
    {
      "id": "82ec97867829f52841e6ad037fbb1e07",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Resa: Transparent Reasoning Models via SAEs",
      "url": "https://arxiv.org/abs/2506.09967",
      "description": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified",
      "published_date": "2025-06-11T13:44:01+00:00",
      "collected_at": "2025-06-13T05:46:49.353650+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "d147b1548d2807a9a7f59e973db9868f",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following",
      "url": "https://arxiv.org/abs/2506.09942",
      "description": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). T",
      "published_date": "2025-06-11T13:10:36+00:00",
      "collected_at": "2025-06-13T05:46:49.356887+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "8d64592190b7d0b198b5e5b3f8df3226",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning",
      "url": "https://arxiv.org/abs/2506.09513",
      "description": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a multi-agent\nverification and refinement process, where we design an Error Refine",
      "published_date": "2025-06-11T04:36:55+00:00",
      "collected_at": "2025-06-13T05:46:49.354978+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "8647cd9cdb434e35eec9fb9d5d10b3c7",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Audio Spectrogram Transformers Beyond the Lab",
      "url": "https://towardsdatascience.com/audio-spectrogram-transformers-beyond-the-lab/",
      "description": "A recipe for building a portable soundscape monitoring app with AudioMoth, Raspberry Pi, and a decent dose of deep learning.\nThe post Audio Spectrogram Transformers Beyond the Lab appeared first on Towards Data Science.",
      "published_date": "2025-06-10T23:47:29+00:00",
      "collected_at": "2025-06-13T05:46:50.707382+00:00",
      "author": "Maciej Adamiak",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "670434083c4a15bec64ebb9e2cdcc44c",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Automate Models Training: An MLOps Pipeline with Tekton and Buildpacks",
      "url": "https://towardsdatascience.com/automate-models-training-an-mlops-pipeline-with-tekton-and-buildpacks/",
      "description": "A step-by-step guide to containerizing and orchestrating an ML training workflow without the Dockerfile headache, using a lightweight GPT-2 example.\nThe post Automate Models Training: An MLOps Pipeline with Tekton and Buildpacks appeared first on Towards Data Science.",
      "published_date": "2025-06-10T23:37:18+00:00",
      "collected_at": "2025-06-13T05:46:50.707585+00:00",
      "author": "Sylvain Kalache",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "14d58c32a594dab10314ce70e770deaf",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "10,000x Faster Bayesian Inference: Multi-GPU SVI vs. Traditional MCMC",
      "url": "https://towardsdatascience.com/10000x-faster-bayesian-inference-multi-gpu-svi-vs-traditional-mcmc/",
      "description": "Using GPU acceleration to speed up Bayesian Inference from months to minutes... \nThe post 10,000x Faster Bayesian Inference: Multi-GPU SVI vs. Traditional MCMC appeared first on Towards Data Science.",
      "published_date": "2025-06-10T23:29:05+00:00",
      "collected_at": "2025-06-13T05:46:50.707780+00:00",
      "author": "Derek Tran",
      "source_priority": 3,
      "score": 92
    },
    {
      "id": "027d866f96062018e82268cc4000a5e6",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation",
      "url": "https://arxiv.org/abs/2506.09344",
      "description": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilita",
      "published_date": "2025-06-10T22:50:49+00:00",
      "collected_at": "2025-06-13T05:46:49.354174+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "50915c04bf618d568192a3eb44ce6fec",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Applications of Density Estimation to Legal Theory",
      "url": "https://towardsdatascience.com/applications-of-density-estimation-to-legal-theory/",
      "description": "A brief analysis using density estimation to compare the two-verdict and three-verdict systems.\nThe post Applications of Density Estimation to Legal Theory appeared first on Towards Data Science.",
      "published_date": "2025-06-10T16:36:24+00:00",
      "collected_at": "2025-06-13T05:46:50.707975+00:00",
      "author": "Jimin Kang",
      "source_priority": 3,
      "score": 92
    },
    {
      "id": "2cba20a73aaee2108868ab012d464b99",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Mastering SQL Window Functions",
      "url": "https://towardsdatascience.com/mastering-sql-window-functions/",
      "description": "Understand how to use Window Functions to perform calculations without losing details\nThe post Mastering SQL Window Functions appeared first on Towards Data Science.",
      "published_date": "2025-06-10T05:36:15+00:00",
      "collected_at": "2025-06-13T05:46:50.708175+00:00",
      "author": "Eugenia Anello",
      "source_priority": 3,
      "score": 91
    },
    {
      "id": "f85347fb0559744e9a6053beb02e544f",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Exploratory Data Analysis: Gamma Spectroscopy in Python",
      "url": "https://towardsdatascience.com/exploratory-data-analysis-gamma-spectroscopy-in-python/",
      "description": "Let’s observe the matter on the atomic level\nThe post Exploratory Data Analysis: Gamma Spectroscopy in Python appeared first on Towards Data Science.",
      "published_date": "2025-06-10T05:27:05+00:00",
      "collected_at": "2025-06-13T05:46:50.708398+00:00",
      "author": "Dmitrii Eliuseev",
      "source_priority": 3,
      "score": 91
    },
    {
      "id": "b5e500a2d2ad993b7c2a8d260dbc2196",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "A Bird’s-Eye View of Linear Algebra: Measure of a Map — Determinants",
      "url": "https://towardsdatascience.com/a-birds-eye-view-of-linear-algebra-measure-of-a-map-determinants/",
      "description": "We roll up our sleeves and start to deal with matrices\nThe post A Bird’s-Eye View of Linear Algebra: Measure of a Map — Determinants appeared first on Towards Data Science.",
      "published_date": "2025-06-10T05:00:27+00:00",
      "collected_at": "2025-06-13T05:46:50.708597+00:00",
      "author": "Rohit Pandey",
      "source_priority": 3,
      "score": 91
    },
    {
      "id": "8974cbf315fc128cbdcbf27fffa994ba",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "How to Transition From Data Analyst to Data Scientist",
      "url": "https://towardsdatascience.com/how-to-transition-from-data-analyst-to-data-scientist/",
      "description": "Playbook on how data analysts can become data scientists\nThe post How to Transition From Data Analyst to Data Scientist appeared first on Towards Data Science.",
      "published_date": "2025-06-09T23:09:55+00:00",
      "collected_at": "2025-06-13T05:46:50.708924+00:00",
      "author": "Egor Howell",
      "source_priority": 3,
      "score": 91
    },
    {
      "id": "5bb2aa15fbc6f8e0fd06eba34a4606b2",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Trying to Stay Sane in the Age of AI",
      "url": "https://towardsdatascience.com/trying-to-stay-sane-in-the-age-of-ai/",
      "description": "A machine learning engineer’s quiet way of not losing her mind\nThe post Trying to Stay Sane in the Age of AI appeared first on Towards Data Science.",
      "published_date": "2025-06-09T22:50:40+00:00",
      "collected_at": "2025-06-13T05:46:50.709126+00:00",
      "author": "Amy Ma",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "da33ba43334b1b294db977ddb78f3380",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Draft-based Approximate Inference for LLMs",
      "url": "https://arxiv.org/abs/2506.08373",
      "description": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV",
      "published_date": "2025-06-09T22:37:46+00:00",
      "collected_at": "2025-06-13T05:46:49.355945+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "1558e855987d2e56712f385cee0dea88",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time\n  Techniques",
      "url": "https://arxiv.org/abs/2506.08060",
      "description": "Large language models have transformed natural language processing, yet\nsupervised fine-tuning (SFT) remains computationally intensive. This paper\nformally proves that capabilities acquired through SFT can be approximated by a\nbase transformer model using inference-time techniques, specifically in-context\nlearning (ICL), without altering model parameters, under idealized assumptions\nincluding unbounded computational resources and access to the fine-tuning\ndataset. We extend these results to prac",
      "published_date": "2025-06-09T04:37:19+00:00",
      "collected_at": "2025-06-13T05:46:49.356662+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "cf3130e8fb5e3059bbebaf5776dcd7b8",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "What Makes a Good Natural Language Prompt?",
      "url": "https://arxiv.org/abs/2506.06950",
      "description": "As large language models (LLMs) have progressed towards more human-like and\nhuman--AI communications have become prevalent, prompting has emerged as a\ndecisive component. However, there is limited conceptual consensus on what\nexactly quantifies natural language prompts. We attempt to address this\nquestion by conducting a meta-analysis surveying more than 150\nprompting-related papers from leading NLP and AI conferences from 2022 to 2025\nand blogs. We propose a property- and human-centric framewor",
      "published_date": "2025-06-07T19:19:27+00:00",
      "collected_at": "2025-06-13T05:46:49.353115+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "e6b9bdd5b896ba5256327cd90e65f9bb",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation\n  Models via Generative Continual Learning",
      "url": "https://arxiv.org/abs/2506.06694",
      "description": "Foundation models have revolutionized fields such as natural language\nprocessing and computer vision by enabling general-purpose learning across\ndiverse tasks and datasets. However, building analogous models for human\nmobility remains challenging due to the privacy-sensitive nature of mobility\ndata and the resulting data silos across institutions. To bridge this gap, we\npropose MoveGCL, a scalable and privacy-preserving framework for training\nmobility foundation models via generative continual l",
      "published_date": "2025-06-07T03:19:11+00:00",
      "collected_at": "2025-06-13T05:46:49.353391+00:00",
      "author": "",
      "source_priority": 1,
      "score": 88
    },
    {
      "id": "ba0a1d0ca488df04266d6f48441d1523",
      "source_id": "pytorch_blog",
      "source": "Blog – PyTorch",
      "category": "Open Source",
      "title": "HuggingFace Safetensors Support in PyTorch Distributed Checkpointing",
      "url": "https://pytorch.org/blog/huggingface-safetensors-support-in-pytorch-distributed-checkpointing/",
      "description": "Summary  PyTorch Distributed Checkpointing (DCP) is making investments into addressing the interoperability blockers to ensure that popular formats, like HuggingFace safetensors, can work well with PyTorch’s ecosystem. Since HuggingFace has...",
      "published_date": "2025-06-06T19:17:46+00:00",
      "collected_at": "2025-06-13T05:46:49.740105+00:00",
      "author": "Ankita George, Saurabh Mishra, Joe Cummings, Philip Bontrager, Daulet Askarov, Teja Rao, Chien-Chin Huang, Ela Krepska, Jafar Taghiyar",
      "source_priority": 2,
      "score": 89
    },
    {
      "id": "d2d2cbaf9e03cb5809f17a9850e7db94",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure\n  Profiles",
      "url": "https://arxiv.org/abs/2506.06561",
      "description": "Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and",
      "published_date": "2025-06-06T18:16:16+00:00",
      "collected_at": "2025-06-13T05:46:49.355483+00:00",
      "author": "",
      "source_priority": 1,
      "score": 96
    },
    {
      "id": "f2e2ef796ae85073f0397c059bcd40ea",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Not Everything Needs Automation: 5 Practical AI Agents That Deliver Enterprise Value",
      "url": "https://towardsdatascience.com/not-everything-needs-automation-5-practical-ai-agents-that-deliver-enterprise-value/",
      "description": "What actually works with AI agents inside enterprise organizations?\nThe post Not Everything Needs Automation: 5 Practical AI Agents That Deliver Enterprise Value appeared first on Towards Data Science.",
      "published_date": "2025-06-06T17:54:12+00:00",
      "collected_at": "2025-06-13T05:46:50.709340+00:00",
      "author": "Weiwei Hu",
      "source_priority": 3,
      "score": 87
    },
    {
      "id": "3d5ee6adb52013963931f8bccef6033c",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Prescriptive Modeling Unpacked: A Complete Guide to Intervention With Bayesian Modeling.",
      "url": "https://towardsdatascience.com/prescriptive-modeling-unpacked-a-complete-guide-to-intervention-with-bayesian-modeling/",
      "description": "Learn how to move beyond prediction and actively make intervention through prescriptive modeling. This in-depth guide walks you through Bayesian approaches to system intervention, with practical examples in predictive maintenance.\nThe post Prescriptive Modeling Unpacked: A Complete Guide to Intervention With Bayesian Modeling. appeared first on Towards Data Science.",
      "published_date": "2025-06-06T17:23:32+00:00",
      "collected_at": "2025-06-13T05:46:50.709536+00:00",
      "author": "Erdogan Taskesen",
      "source_priority": 3,
      "score": 87
    },
    {
      "id": "1baeabdb760d5d46d8825a522a214e3e",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "How I Automated My Machine Learning Workflow with Just 10 Lines of Python",
      "url": "https://towardsdatascience.com/how-i-automated-my-machine-learning-workflow-with-just-10-lines-of-python/",
      "description": "Use LazyPredict and PyCaret to skip the grunt work and jump straight to performance.\nThe post How I Automated My Machine Learning Workflow with Just 10 Lines of Python appeared first on Towards Data Science.",
      "published_date": "2025-06-06T13:11:46+00:00",
      "collected_at": "2025-06-13T05:46:50.709726+00:00",
      "author": "Himanshu Sharma",
      "source_priority": 3,
      "score": 99
    },
    {
      "id": "d2699af6a19cabc236f6300f9e9cffbe",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks",
      "url": "https://arxiv.org/abs/2506.05982",
      "description": "As automated attack techniques rapidly advance, CAPTCHAs remain a critical\ndefense mechanism against malicious bots. However, existing CAPTCHA schemes\nencompass a diverse range of modalities -- from static distorted text and\nobfuscated images to interactive clicks, sliding puzzles, and logic-based\nquestions -- yet the community still lacks a unified, large-scale, multimodal\nbenchmark to rigorously evaluate their security robustness. To address this\ngap, we introduce MCA-Bench, a comprehensive an",
      "published_date": "2025-06-06T07:02:01+00:00",
      "collected_at": "2025-06-13T05:46:49.356175+00:00",
      "author": "",
      "source_priority": 1,
      "score": 96
    }
  ],
  "updated": "2025-06-13T05:47:06.581557"
}