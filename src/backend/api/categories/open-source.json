{
  "category": "Open Source",
  "count": 40,
  "articles": [
    {
      "id": "0982c8bdf5f0c9191bd5ac8f4ecf7bf5",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Stop Building AI Platforms",
      "url": "https://towardsdatascience.com/stop-building-ai-platforms/",
      "description": "When small and medium companies achieve success in building Data and ML platforms, building AI platforms is now profoundly challenging\nThe post Stop Building AI Platforms appeared first on Towards Data Science.",
      "published_date": "2025-06-14T01:26:49+00:00",
      "collected_at": "2025-06-15T01:25:04.680681+00:00",
      "author": "Ming Gao",
      "source_priority": 3,
      "score": 94
    },
    {
      "id": "58f37280e0df15f573bc141bb368f7e0",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "What If I had AI in 2018: Rent the Runway Fulfillment Center Optimization",
      "url": "https://towardsdatascience.com/what-if-i-had-ai-in-2018-rent-the-runway-fulfillment-center-optimization/",
      "description": "An LLM in 2018 would not have trivialized a complex project, although it could have enhanced the final solution\nThe post What If I had AI in 2018: Rent the Runway Fulfillment Center Optimization appeared first on Towards Data Science.",
      "published_date": "2025-06-13T23:03:03+00:00",
      "collected_at": "2025-06-15T01:25:04.680895+00:00",
      "author": "Hugo Ducruc",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "08e4357cebd144a10ee62a390d9691a5",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "AI Is Not a Black Box (Relatively Speaking)",
      "url": "https://towardsdatascience.com/ai-is-not-a-black-box/",
      "description": "Compared to the opacity around human intelligence, AI is more transparent in some very tangible ways.\nThe post AI Is Not a Black Box (Relatively Speaking) appeared first on Towards Data Science.",
      "published_date": "2025-06-13T20:02:51+00:00",
      "collected_at": "2025-06-15T01:25:04.681098+00:00",
      "author": "Piotr (Peter) Mardziel",
      "source_priority": 3,
      "score": 94
    },
    {
      "id": "a47ee4e7d65a00ac2f3bffd11bc261a4",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "How AI Agents “Talk” to Each Other",
      "url": "https://towardsdatascience.com/how-ai-agents-talk-to-each-other/",
      "description": "Minimize chaos and maintain inter-agent harmony in your projects\nThe post How AI Agents “Talk” to Each Other appeared first on Towards Data Science.",
      "published_date": "2025-06-13T19:21:14+00:00",
      "collected_at": "2025-06-15T01:25:04.681311+00:00",
      "author": "TDS Editors",
      "source_priority": 3,
      "score": 94
    },
    {
      "id": "60c1fe34ea4d768996bae1b32a454c16",
      "source_id": "pytorch_blog",
      "source": "Blog – PyTorch",
      "category": "Open Source",
      "title": "ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization",
      "url": "https://pytorch.org/blog/paretoq-scaling-laws-in-extremely-low-bit-llm-quantization/",
      "description": "The field of large language models is shifting toward lower-precision computation. This shift necessitates a rethinking of scaling laws to account for the effects of quantization on resulting quantized model...",
      "published_date": "2025-06-13T18:43:38+00:00",
      "collected_at": "2025-06-15T01:25:03.077879+00:00",
      "author": "Zechun Liu, Changsheng Zhao, Hanxian Huang, Sijia Chen, Jing Zhang, Andrew Or, Jiawei Zhao, Scott Roy, Lisa Jin, Yunyang Xiong, Yangyang Shi, Lin Xiao, Yuandong Tian, Bilge Soran, Raghuraman Krishnamoorthi, Tijmen Blankevoort, Vikas Chandra (Meta)",
      "source_priority": 2,
      "score": 100
    },
    {
      "id": "2d83a9cb9a328057c618264033ae1dbe",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Connecting the Dots for Better Movie Recommendations",
      "url": "https://towardsdatascience.com/connecting-the-dots-for-better-movie-recommendations/",
      "description": "Connecting the Dots for Better Movie Recommendations: Lightweight graph RAG on Rotten Tomatoes movie reviews\nThe post Connecting the Dots for Better Movie Recommendations appeared first on Towards Data Science.",
      "published_date": "2025-06-13T00:27:55+00:00",
      "collected_at": "2025-06-15T01:25:04.681506+00:00",
      "author": "Brian Godsey",
      "source_priority": 3,
      "score": 93
    },
    {
      "id": "04bff2841ad31c6f572d7fe153ed41d4",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Agentic AI 103: Building Multi-Agent Teams",
      "url": "https://towardsdatascience.com/agentic-ai-103-building-multi-agent-teams/",
      "description": "Build multi-agent teams that can automate tasks and enhance productivity.\nThe post Agentic AI 103: Building Multi-Agent Teams appeared first on Towards Data Science.",
      "published_date": "2025-06-12T19:34:10+00:00",
      "collected_at": "2025-06-15T01:25:04.681718+00:00",
      "author": "Gustavo Santos",
      "source_priority": 3,
      "score": 92
    },
    {
      "id": "7a26e350ec7d0152163a44bb1175001a",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Design Smarter Prompts and Boost Your LLM Output: Real Tricks from an AI Engineer’s Toolbox",
      "url": "https://towardsdatascience.com/boost-your-llm-outputdesign-smarter-prompts-real-tricks-from-an-ai-engineers-toolbox/",
      "description": "Not just what you ask, but how you ask it. Practical techniques for prompt engineering that deliver\nThe post Design Smarter Prompts and Boost Your LLM Output: Real Tricks from an AI Engineer’s Toolbox appeared first on Towards Data Science.",
      "published_date": "2025-06-12T18:54:58+00:00",
      "collected_at": "2025-06-15T01:25:04.681920+00:00",
      "author": "Ugo Pradère",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "5d0a0ada767700d7cc88a28b0c2d8da5",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "User Authorisation in Streamlit With OIDC and Google",
      "url": "https://towardsdatascience.com/user-authorisation-in-streamlit-with-oidc-and-google/",
      "description": "Log in to a Streamlit app with a Google email account\nThe post User Authorisation in Streamlit With OIDC and Google appeared first on Towards Data Science.",
      "published_date": "2025-06-12T16:54:07+00:00",
      "collected_at": "2025-06-15T01:25:04.682116+00:00",
      "author": "Thomas Reid",
      "source_priority": 3,
      "score": 92
    },
    {
      "id": "2a7b12e559517d107b6a58bb982d8c13",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Fine-Grained Perturbation Guidance via Attention Head Selection",
      "url": "https://arxiv.org/abs/2506.10978",
      "description": "Recent guidance methods in diffusion models steer reverse sampling by\nperturbing the model to construct an implicit weak model and guide generation\naway from it. Among these approaches, attention perturbation has demonstrated\nstrong empirical performance in unconditional scenarios where classifier-free\nguidance is not applicable. However, existing attention perturbation methods\nlack principled approaches for determining where perturbations should be\napplied, particularly in Diffusion Transformer",
      "published_date": "2025-06-12T13:59:51+00:00",
      "collected_at": "2025-06-15T01:25:02.929623+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "936f31fc98bda2bcd4d5f358e69af710",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
      "url": "https://arxiv.org/abs/2506.10974",
      "description": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex,",
      "published_date": "2025-06-12T13:59:32+00:00",
      "collected_at": "2025-06-15T01:25:02.932083+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "9cafde8b8a7555e3ba800d52f698c89f",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Decomposing MLP Activations into Interpretable Features via\n  Semi-Nonnegative Matrix Factorization",
      "url": "https://arxiv.org/abs/2506.10920",
      "description": "A central goal for mechanistic interpretability has been to identify the\nright units of analysis in large language models (LLMs) that causally explain\ntheir outputs. While early work focused on individual neurons, evidence that\nneurons often encode multiple concepts has motivated a shift toward analyzing\ndirections in activation space. A key question is how to find directions that\ncapture interpretable features in an unsupervised manner. Current methods rely\non dictionary learning with sparse au",
      "published_date": "2025-06-12T13:33:29+00:00",
      "collected_at": "2025-06-15T01:25:02.928074+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "5eecf2cbb485a5309aec56421af3e134",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "NoLoCo: No-all-reduce Low Communication Training Method for Large Models",
      "url": "https://arxiv.org/abs/2506.10911",
      "description": "Training large language models is generally done via optimization methods on\nclusters containing tens of thousands of accelerators, communicating over a\nhigh-bandwidth interconnect. Scaling up these clusters is expensive and can\nbecome impractical, imposing limits on the size of models that can be trained.\nSeveral recent studies have proposed training methods that are less\ncommunication intensive, avoiding the need for a highly connected compute\ncluster. These state-of-the-art low communication",
      "published_date": "2025-06-12T13:23:23+00:00",
      "collected_at": "2025-06-15T01:25:02.927491+00:00",
      "author": "",
      "source_priority": 1,
      "score": 93
    },
    {
      "id": "6c940bb1aef5155d7e2fabc99e7bc290",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Magistral",
      "url": "https://arxiv.org/abs/2506.10910",
      "description": "We introduce Magistral, Mistral's first reasoning model and our own scalable\nreinforcement learning (RL) pipeline. Instead of relying on existing\nimplementations and RL traces distilled from prior models, we follow a ground\nup approach, relying solely on our own models and infrastructure. Notably, we\ndemonstrate a stack that enabled us to explore the limits of pure RL training\nof LLMs, present a simple method to force the reasoning language of the model,\nand show that RL on text data alone maint",
      "published_date": "2025-06-12T13:22:37+00:00",
      "collected_at": "2025-06-15T01:25:02.931857+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "3998518fd9cfc7664cde649cd2d86bb9",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a\n  Unified Framework",
      "url": "https://arxiv.org/abs/2506.10741",
      "description": "Generating aesthetic posters is more challenging than simple design images:\nit requires not only precise text rendering but also the seamless integration\nof abstract artistic content, striking layouts, and overall stylistic harmony.\nTo address this, we propose PosterCraft, a unified framework that abandons\nprior modular pipelines and rigid, predefined layouts, allowing the model to\nfreely explore coherent, visually compelling compositions. PosterCraft employs\na carefully designed, cascaded workf",
      "published_date": "2025-06-12T10:28:12+00:00",
      "collected_at": "2025-06-15T01:25:02.931610+00:00",
      "author": "",
      "source_priority": 1,
      "score": 93
    },
    {
      "id": "d1bba51b852d5b0ae929c22c209e66a1",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to\n  Evolving Research Corpora",
      "url": "https://arxiv.org/abs/2506.10737",
      "description": "The rapid evolution of scientific fields introduces challenges in organizing\nand retrieving scientific literature. While expert-curated taxonomies have\ntraditionally addressed this need, the process is time-consuming and expensive.\nFurthermore, recent automatic taxonomy construction methods either (1)\nover-rely on a specific corpus, sacrificing generalizability, or (2) depend\nheavily on the general knowledge of large language models (LLMs) contained\nwithin their pre-training datasets, often over",
      "published_date": "2025-06-12T10:26:28+00:00",
      "collected_at": "2025-06-15T01:25:02.928853+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "1fc4ec3290b623615b92afe95da6707d",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Beyond True or False: Retrieval-Augmented Hierarchical Analysis of\n  Nuanced Claims",
      "url": "https://arxiv.org/abs/2506.10728",
      "description": "Claims made by individuals or entities are oftentimes nuanced and cannot be\nclearly labeled as entirely \"true\" or \"false\" -- as is frequently the case with\nscientific and political claims. However, a claim (e.g., \"vaccine A is better\nthan vaccine B\") can be dissected into its integral aspects and sub-aspects\n(e.g., efficacy, safety, distribution), which are individually easier to\nvalidate. This enables a more comprehensive, structured response that provides\na well-rounded perspective on a given",
      "published_date": "2025-06-12T10:17:45+00:00",
      "collected_at": "2025-06-15T01:25:02.929129+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "105cc7bcb4d3046a938205d469753c47",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "TeleMath: A Benchmark for Large Language Models in Telecom Mathematical\n  Problem Solving",
      "url": "https://arxiv.org/abs/2506.10674",
      "description": "The increasing adoption of artificial intelligence in telecommunications has\nraised interest in the capability of Large Language Models (LLMs) to address\ndomain-specific, mathematically intensive tasks. Although recent advancements\nhave improved the performance of LLMs in general mathematical reasoning, their\neffectiveness within specialized domains, such as signal processing, network\noptimization, and performance analysis, remains largely unexplored. To address\nthis gap, we introduce TeleMath,",
      "published_date": "2025-06-12T09:04:18+00:00",
      "collected_at": "2025-06-15T01:25:02.928582+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "f44b9f54b573fa0a32ab8f4c176ab773",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "EmbodiedGen: Towards a Generative 3D World Engine for Embodied\n  Intelligence",
      "url": "https://arxiv.org/abs/2506.10600",
      "description": "Constructing a physically realistic and accurately scaled simulated 3D world\nis crucial for the training and evaluation of embodied intelligence tasks. The\ndiversity, realism, low cost accessibility and affordability of 3D data assets\nare critical for achieving generalization and scalability in embodied AI.\nHowever, most current embodied intelligence tasks still rely heavily on\ntraditional 3D computer graphics assets manually created and annotated, which\nsuffer from high production costs and lim",
      "published_date": "2025-06-12T07:43:50+00:00",
      "collected_at": "2025-06-15T01:25:02.928330+00:00",
      "author": "",
      "source_priority": 1,
      "score": 93
    },
    {
      "id": "a23da3466e7495d3a04feef729a4b506",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "DreamActor-H1: High-Fidelity Human-Product Demonstration Video\n  Generation via Motion-designed Diffusion Transformers",
      "url": "https://arxiv.org/abs/2506.10568",
      "description": "In e-commerce and digital marketing, generating high-fidelity human-product\ndemonstration videos is important for effective product presentation. However,\nmost existing frameworks either fail to preserve the identities of both humans\nand products or lack an understanding of human-product spatial relationships,\nleading to unrealistic representations and unnatural interactions. To address\nthese challenges, we propose a Diffusion Transformer (DiT)-based framework. Our\nmethod simultaneously preserve",
      "published_date": "2025-06-12T06:58:23+00:00",
      "collected_at": "2025-06-15T01:25:02.929881+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "c1685dfb6d0b3dcff3b54be485a863fb",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Exploring the Proportional Odds Model for Ordinal Logistic Regression",
      "url": "https://towardsdatascience.com/proportional-odds-model-for-ordinal-logistic-regression/",
      "description": "Understanding and Implementing Brant’s Tests in Ordinal Logistic Regression with Python\nThe post Exploring the Proportional Odds Model for Ordinal Logistic Regression appeared first on Towards Data Science.",
      "published_date": "2025-06-12T05:45:56+00:00",
      "collected_at": "2025-06-15T01:25:04.682315+00:00",
      "author": "JUNIOR JUMBONG",
      "source_priority": 3,
      "score": 91
    },
    {
      "id": "9063e838e64e6d653b3129520f884b40",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Can AI Truly Develop a Memory That Adapts Like Ours?",
      "url": "https://towardsdatascience.com/can-ai-truly-develop-a-memory-that-adapts-like-ours/",
      "description": "Exploring Titans: A new architecture equipping LLMs with human-inspired memory that learns and updates itself during test-time.\nThe post Can AI Truly Develop a Memory That Adapts Like Ours? appeared first on Towards Data Science.",
      "published_date": "2025-06-12T05:32:11+00:00",
      "collected_at": "2025-06-15T01:25:04.682510+00:00",
      "author": "Moulik Gupta",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "3de602462b42e0e2c00fc5df6a06a4b6",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Discrete Audio Tokens: More Than a Survey!",
      "url": "https://arxiv.org/abs/2506.10274",
      "description": "Discrete audio tokens are compact representations that aim to preserve\nperceptual quality, phonetic content, and speaker characteristics while\nenabling efficient storage and inference, as well as competitive performance\nacross diverse downstream tasks.They provide a practical alternative to\ncontinuous features, enabling the integration of speech and audio into modern\nlarge language models (LLMs). As interest in token-based audio processing\ngrows, various tokenization methods have emerged, and se",
      "published_date": "2025-06-11T21:35:43+00:00",
      "collected_at": "2025-06-15T01:25:02.929381+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "f6d81dda61f177f4ac4a259da522fa1d",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Model Context Protocol (MCP) Tutorial: Build Your First MCP Server in 6 Steps",
      "url": "https://towardsdatascience.com/model-context-protocol-mcp-tutorial-build-your-first-mcp-server-in-6-steps/",
      "description": "A beginner-friendly tutorial of MCP architecture, with the focus on MCP server components and applications, guiding through the process of building a custom MCP server that enables code-to-diagram.\nThe post Model Context Protocol (MCP) Tutorial: Build Your First MCP Server in 6 Steps appeared first on Towards Data Science.",
      "published_date": "2025-06-11T19:40:45+00:00",
      "collected_at": "2025-06-15T01:25:04.682721+00:00",
      "author": "Destin Gong",
      "source_priority": 3,
      "score": 91
    },
    {
      "id": "68c07722e8ce23d92d28c6f82a57c3be",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Attention, Please! Revisiting Attentive Probing for Masked Image\n  Modeling",
      "url": "https://arxiv.org/abs/2506.10178",
      "description": "As fine-tuning (FT) becomes increasingly impractical at scale, probing is\nemerging as the preferred evaluation protocol for self-supervised learning\n(SSL). Yet, the standard linear probing (LP) fails to adequately reflect the\npotential of models trained with Masked Image Modeling (MIM), due to the\ndistributed nature of patch tokens. This motivates the need for attentive\nprobing, an alternative that uses attention to selectively aggregate\npatch-level features. Despite its growing adoption, attent",
      "published_date": "2025-06-11T17:10:26+00:00",
      "collected_at": "2025-06-15T01:25:02.930116+00:00",
      "author": "",
      "source_priority": 1,
      "score": 92
    },
    {
      "id": "036d3d830f2eae87fdae67f04880d8b8",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Mobile App Development with Python",
      "url": "https://towardsdatascience.com/mobile-app-development-with-python/",
      "description": "Build iOS & Android Apps with Kivy\nThe post Mobile App Development with Python appeared first on Towards Data Science.",
      "published_date": "2025-06-11T16:55:03+00:00",
      "collected_at": "2025-06-15T01:25:04.731748+00:00",
      "author": "Mauro Di Pietro",
      "source_priority": 3,
      "score": 91
    },
    {
      "id": "732cb140e94759e87081b3ba3b69e428",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal\n  Gaussian Splatting",
      "url": "https://arxiv.org/abs/2506.09952",
      "description": "The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primit",
      "published_date": "2025-06-11T13:23:21+00:00",
      "collected_at": "2025-06-15T01:25:02.930905+00:00",
      "author": "",
      "source_priority": 1,
      "score": 92
    },
    {
      "id": "8647cd9cdb434e35eec9fb9d5d10b3c7",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Audio Spectrogram Transformers Beyond the Lab",
      "url": "https://towardsdatascience.com/audio-spectrogram-transformers-beyond-the-lab/",
      "description": "A recipe for building a portable soundscape monitoring app with AudioMoth, Raspberry Pi, and a decent dose of deep learning.\nThe post Audio Spectrogram Transformers Beyond the Lab appeared first on Towards Data Science.",
      "published_date": "2025-06-10T23:47:29+00:00",
      "collected_at": "2025-06-15T01:25:04.731970+00:00",
      "author": "Maciej Adamiak",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "670434083c4a15bec64ebb9e2cdcc44c",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Automate Models Training: An MLOps Pipeline with Tekton and Buildpacks",
      "url": "https://towardsdatascience.com/automate-models-training-an-mlops-pipeline-with-tekton-and-buildpacks/",
      "description": "A step-by-step guide to containerizing and orchestrating an ML training workflow without the Dockerfile headache, using a lightweight GPT-2 example.\nThe post Automate Models Training: An MLOps Pipeline with Tekton and Buildpacks appeared first on Towards Data Science.",
      "published_date": "2025-06-10T23:37:18+00:00",
      "collected_at": "2025-06-15T01:25:04.732297+00:00",
      "author": "Sylvain Kalache",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "14d58c32a594dab10314ce70e770deaf",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "10,000x Faster Bayesian Inference: Multi-GPU SVI vs. Traditional MCMC",
      "url": "https://towardsdatascience.com/10000x-faster-bayesian-inference-multi-gpu-svi-vs-traditional-mcmc/",
      "description": "Using GPU acceleration to speed up Bayesian Inference from months to minutes... \nThe post 10,000x Faster Bayesian Inference: Multi-GPU SVI vs. Traditional MCMC appeared first on Towards Data Science.",
      "published_date": "2025-06-10T23:29:05+00:00",
      "collected_at": "2025-06-15T01:25:04.732493+00:00",
      "author": "Derek Tran",
      "source_priority": 3,
      "score": 90
    },
    {
      "id": "a28c0119414440455448e512f68a2418",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Token Perturbation Guidance for Diffusion Models",
      "url": "https://arxiv.org/abs/2506.10036",
      "description": "Classifier-free guidance (CFG) has become an essential component of modern\ndiffusion models to enhance both generation quality and alignment with input\nconditions. However, CFG requires specific training procedures and is limited\nto conditional generation. To address these limitations, we propose Token\nPerturbation Guidance (TPG), a novel method that applies perturbation matrices\ndirectly to intermediate token representations within the diffusion network.\nTPG employs a norm-preserving shuffling",
      "published_date": "2025-06-10T17:25:46+00:00",
      "collected_at": "2025-06-15T01:25:02.931380+00:00",
      "author": "",
      "source_priority": 1,
      "score": 91
    },
    {
      "id": "285b09747cd48262d9f5110a5e759c54",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Comment on The Illusion of Thinking: Understanding the Strengths and\n  Limitations of Reasoning Models via the Lens of Problem Complexity",
      "url": "https://arxiv.org/abs/2506.09250",
      "description": "Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit\n\"accuracy collapse\" on planning puzzles beyond certain complexity thresholds.\nWe demonstrate that their findings primarily reflect experimental design\nlimitations rather than fundamental reasoning failures. Our analysis reveals\nthree critical issues: (1) Tower of Hanoi experiments systematically exceed\nmodel output token limits at reported failure points, with models explicitly\nacknowledging these constraints in their output",
      "published_date": "2025-06-10T17:16:53+00:00",
      "collected_at": "2025-06-15T01:25:02.927245+00:00",
      "author": "",
      "source_priority": 1,
      "score": 91
    },
    {
      "id": "50915c04bf618d568192a3eb44ce6fec",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Applications of Density Estimation to Legal Theory",
      "url": "https://towardsdatascience.com/applications-of-density-estimation-to-legal-theory/",
      "description": "A brief analysis using density estimation to compare the two-verdict and three-verdict systems.\nThe post Applications of Density Estimation to Legal Theory appeared first on Towards Data Science.",
      "published_date": "2025-06-10T16:36:24+00:00",
      "collected_at": "2025-06-15T01:25:04.732712+00:00",
      "author": "Jimin Kang",
      "source_priority": 3,
      "score": 89
    },
    {
      "id": "247080e80e8edd61863173b9d9f4ac59",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated\n  Video Streams",
      "url": "https://arxiv.org/abs/2506.08862",
      "description": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams\nis crucial for numerous real-world applications. However, existing methods\nstruggle to jointly address three key challenges: 1) processing uncalibrated\ninputs in real time, 2) accurately modeling dynamic scene evolution, and 3)\nmaintaining long-term stability and computational efficiency. To this end, we\nintroduce StreamSplat, the first fully feed-forward framework that transforms\nuncalibrated video streams of arbitrar",
      "published_date": "2025-06-10T10:52:36+00:00",
      "collected_at": "2025-06-15T01:25:02.930413+00:00",
      "author": "",
      "source_priority": 1,
      "score": 90
    },
    {
      "id": "2cba20a73aaee2108868ab012d464b99",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Mastering SQL Window Functions",
      "url": "https://towardsdatascience.com/mastering-sql-window-functions/",
      "description": "Understand how to use Window Functions to perform calculations without losing details\nThe post Mastering SQL Window Functions appeared first on Towards Data Science.",
      "published_date": "2025-06-10T05:36:15+00:00",
      "collected_at": "2025-06-15T01:25:04.732906+00:00",
      "author": "Eugenia Anello",
      "source_priority": 3,
      "score": 89
    },
    {
      "id": "f85347fb0559744e9a6053beb02e544f",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Exploratory Data Analysis: Gamma Spectroscopy in Python",
      "url": "https://towardsdatascience.com/exploratory-data-analysis-gamma-spectroscopy-in-python/",
      "description": "Let’s observe the matter on the atomic level\nThe post Exploratory Data Analysis: Gamma Spectroscopy in Python appeared first on Towards Data Science.",
      "published_date": "2025-06-10T05:27:05+00:00",
      "collected_at": "2025-06-15T01:25:04.733100+00:00",
      "author": "Dmitrii Eliuseev",
      "source_priority": 3,
      "score": 89
    },
    {
      "id": "b5e500a2d2ad993b7c2a8d260dbc2196",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "A Bird’s-Eye View of Linear Algebra: Measure of a Map — Determinants",
      "url": "https://towardsdatascience.com/a-birds-eye-view-of-linear-algebra-measure-of-a-map-determinants/",
      "description": "We roll up our sleeves and start to deal with matrices\nThe post A Bird’s-Eye View of Linear Algebra: Measure of a Map — Determinants appeared first on Towards Data Science.",
      "published_date": "2025-06-10T05:00:27+00:00",
      "collected_at": "2025-06-15T01:25:04.733293+00:00",
      "author": "Rohit Pandey",
      "source_priority": 3,
      "score": 89
    },
    {
      "id": "8974cbf315fc128cbdcbf27fffa994ba",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "How to Transition From Data Analyst to Data Scientist",
      "url": "https://towardsdatascience.com/how-to-transition-from-data-analyst-to-data-scientist/",
      "description": "Playbook on how data analysts can become data scientists\nThe post How to Transition From Data Analyst to Data Scientist appeared first on Towards Data Science.",
      "published_date": "2025-06-09T23:09:55+00:00",
      "collected_at": "2025-06-15T01:25:04.733483+00:00",
      "author": "Egor Howell",
      "source_priority": 3,
      "score": 89
    },
    {
      "id": "93f37a4ba94dd7229d84452a6441c9f6",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and\n  Future Directions",
      "url": "https://arxiv.org/abs/2506.08234",
      "description": "Recent advancements in large language models (LLMs) and AI systems have led\nto a paradigm shift in the design and optimization of complex AI workflows. By\nintegrating multiple components, compound AI systems have become increasingly\nadept at performing sophisticated tasks. However, as these systems grow in\ncomplexity, new challenges arise in optimizing not only individual components\nbut also their interactions. While traditional optimization methods such as\nsupervised fine-tuning (SFT) and reinf",
      "published_date": "2025-06-09T17:04:14+00:00",
      "collected_at": "2025-06-15T01:25:02.930671+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "bff69df57dc22868213189637a98e9ba",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "LLM Unlearning Should Be Form-Independent",
      "url": "https://arxiv.org/abs/2506.07795",
      "description": "Large Language Model (LLM) unlearning aims to erase or suppress undesirable\nknowledge within the model, offering promise for controlling harmful or private\ninformation to prevent misuse. However, recent studies highlight its limited\nefficacy in real-world scenarios, hindering practical adoption. In this study,\nwe identify a pervasive issue underlying many downstream failures: the\neffectiveness of existing unlearning methods heavily depends on the form of\ntraining samples and frequently fails to",
      "published_date": "2025-06-09T10:21:25+00:00",
      "collected_at": "2025-06-15T01:25:02.931145+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    }
  ],
  "updated": "2025-06-15T01:25:49.360826"
}