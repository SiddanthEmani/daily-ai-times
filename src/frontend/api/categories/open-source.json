{
  "category": "Open Source",
  "count": 41,
  "articles": [
    {
      "id": "2d83a9cb9a328057c618264033ae1dbe",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Connecting the Dots for Better Movie Recommendations",
      "url": "https://towardsdatascience.com/connecting-the-dots-for-better-movie-recommendations/",
      "description": "Connecting the Dots for Better Movie Recommendations: Lightweight graph RAG on Rotten Tomatoes movie reviews\nThe post Connecting the Dots for Better Movie Recommendations appeared first on Towards Data Science.",
      "published_date": "2025-06-13T00:27:55+00:00",
      "collected_at": "2025-06-13T12:33:45.488326+00:00",
      "author": "Brian Godsey",
      "source_priority": 3,
      "score": 95
    },
    {
      "id": "04bff2841ad31c6f572d7fe153ed41d4",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Agentic AI 103: Building Multi-Agent Teams",
      "url": "https://towardsdatascience.com/agentic-ai-103-building-multi-agent-teams/",
      "description": "Build multi-agent teams that can automate tasks and enhance productivity.\nThe post Agentic AI 103: Building Multi-Agent Teams appeared first on Towards Data Science.",
      "published_date": "2025-06-12T19:34:10+00:00",
      "collected_at": "2025-06-13T12:33:45.488536+00:00",
      "author": "Gustavo Santos",
      "source_priority": 3,
      "score": 94
    },
    {
      "id": "7a26e350ec7d0152163a44bb1175001a",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Design Smarter Prompts and Boost Your LLM Output: Real Tricks from an AI Engineer’s Toolbox",
      "url": "https://towardsdatascience.com/boost-your-llm-outputdesign-smarter-prompts-real-tricks-from-an-ai-engineers-toolbox/",
      "description": "Not just what you ask, but how you ask it. Practical techniques for prompt engineering that deliver\nThe post Design Smarter Prompts and Boost Your LLM Output: Real Tricks from an AI Engineer’s Toolbox appeared first on Towards Data Science.",
      "published_date": "2025-06-12T18:54:58+00:00",
      "collected_at": "2025-06-13T12:33:45.488749+00:00",
      "author": "Ugo Pradère",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "5d0a0ada767700d7cc88a28b0c2d8da5",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "User Authorisation in Streamlit With OIDC and Google",
      "url": "https://towardsdatascience.com/user-authorisation-in-streamlit-with-oidc-and-google/",
      "description": "Log in to a Streamlit app with a Google email account\nThe post User Authorisation in Streamlit With OIDC and Google appeared first on Towards Data Science.",
      "published_date": "2025-06-12T16:54:07+00:00",
      "collected_at": "2025-06-13T12:33:45.488975+00:00",
      "author": "Thomas Reid",
      "source_priority": 3,
      "score": 94
    },
    {
      "id": "2a7b12e559517d107b6a58bb982d8c13",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Fine-Grained Perturbation Guidance via Attention Head Selection",
      "url": "https://arxiv.org/abs/2506.10978",
      "description": "Recent guidance methods in diffusion models steer reverse sampling by\nperturbing the model to construct an implicit weak model and guide generation\naway from it. Among these approaches, attention perturbation has demonstrated\nstrong empirical performance in unconditional scenarios where classifier-free\nguidance is not applicable. However, existing attention perturbation methods\nlack principled approaches for determining where perturbations should be\napplied, particularly in Diffusion Transformer",
      "published_date": "2025-06-12T13:59:51+00:00",
      "collected_at": "2025-06-13T12:33:44.458201+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "936f31fc98bda2bcd4d5f358e69af710",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
      "url": "https://arxiv.org/abs/2506.10974",
      "description": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex,",
      "published_date": "2025-06-12T13:59:32+00:00",
      "collected_at": "2025-06-13T12:33:44.460981+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "5bbff26f9ec82300f61da365ff122f59",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark",
      "url": "https://arxiv.org/abs/2506.10960",
      "description": "Large language models (LLMs) have been increasingly applied to automated\nharmful content detection tasks, assisting moderators in identifying policy\nviolations and improving the overall efficiency and accuracy of content review.\nHowever, existing resources for harmful content detection are predominantly\nfocused on English, with Chinese datasets remaining scarce and often limited in\nscope. We present a comprehensive, professionally annotated benchmark for\nChinese content harm detection, which cov",
      "published_date": "2025-06-12T13:57:05+00:00",
      "collected_at": "2025-06-13T12:33:44.461214+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "7a8f9ad549edffa35b8df98f49986c71",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Build the web for agents, not agents for the web",
      "url": "https://arxiv.org/abs/2506.10953",
      "description": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of",
      "published_date": "2025-06-12T13:53:58+00:00",
      "collected_at": "2025-06-13T12:33:44.462636+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "79eca08b5853abc5578275b822338497",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture\n  without Training",
      "url": "https://arxiv.org/abs/2506.10952",
      "description": "We introduce~Domain2Vec, a novel approach that decomposes any\ndataset into a linear combination of several meta-domains, a new concept\ndesigned to capture the key underlying features of datasets.\nDomain2Vec maintains a vocabulary of meta-domains and uses a\nclassifier to decompose any given dataset into a domain vector that corresponds\nto a distribution over this vocabulary. These domain vectors enable the\nidentification of the optimal data mixture for language model (LM) pretraining\nin a trainin",
      "published_date": "2025-06-12T13:53:51+00:00",
      "collected_at": "2025-06-13T12:33:44.463139+00:00",
      "author": "",
      "source_priority": 1,
      "score": 95
    },
    {
      "id": "6c940bb1aef5155d7e2fabc99e7bc290",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Magistral",
      "url": "https://arxiv.org/abs/2506.10910",
      "description": "We introduce Magistral, Mistral's first reasoning model and our own scalable\nreinforcement learning (RL) pipeline. Instead of relying on existing\nimplementations and RL traces distilled from prior models, we follow a ground\nup approach, relying solely on our own models and infrastructure. Notably, we\ndemonstrate a stack that enabled us to explore the limits of pure RL training\nof LLMs, present a simple method to force the reasoning language of the model,\nand show that RL on text data alone maint",
      "published_date": "2025-06-12T13:22:37+00:00",
      "collected_at": "2025-06-13T12:33:44.460722+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "3998518fd9cfc7664cde649cd2d86bb9",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a\n  Unified Framework",
      "url": "https://arxiv.org/abs/2506.10741",
      "description": "Generating aesthetic posters is more challenging than simple design images:\nit requires not only precise text rendering but also the seamless integration\nof abstract artistic content, striking layouts, and overall stylistic harmony.\nTo address this, we propose PosterCraft, a unified framework that abandons\nprior modular pipelines and rigid, predefined layouts, allowing the model to\nfreely explore coherent, visually compelling compositions. PosterCraft employs\na carefully designed, cascaded workf",
      "published_date": "2025-06-12T10:28:12+00:00",
      "collected_at": "2025-06-13T12:33:44.460480+00:00",
      "author": "",
      "source_priority": 1,
      "score": 95
    },
    {
      "id": "a23da3466e7495d3a04feef729a4b506",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "DreamActor-H1: High-Fidelity Human-Product Demonstration Video\n  Generation via Motion-designed Diffusion Transformers",
      "url": "https://arxiv.org/abs/2506.10568",
      "description": "In e-commerce and digital marketing, generating high-fidelity human-product\ndemonstration videos is important for effective product presentation. However,\nmost existing frameworks either fail to preserve the identities of both humans\nand products or lack an understanding of human-product spatial relationships,\nleading to unrealistic representations and unnatural interactions. To address\nthese challenges, we propose a Diffusion Transformer (DiT)-based framework. Our\nmethod simultaneously preserve",
      "published_date": "2025-06-12T06:58:23+00:00",
      "collected_at": "2025-06-13T12:33:44.458503+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "4c6f175ac4fdbeb60ce5b925c15d0ca6",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation",
      "url": "https://arxiv.org/abs/2506.10540",
      "description": "Despite rapid advancements in video generation models, generating coherent\nstorytelling videos that span multiple scenes and characters remains\nchallenging. Current methods often rigidly convert pre-generated keyframes into\nfixed-length clips, resulting in disjointed narratives and pacing issues.\nFurthermore, the inherent instability of video generation models means that\neven a single low-quality clip can significantly degrade the entire output\nanimation's logical coherence and visual continuity",
      "published_date": "2025-06-12T06:06:21+00:00",
      "collected_at": "2025-06-13T12:33:44.461687+00:00",
      "author": "",
      "source_priority": 1,
      "score": 95
    },
    {
      "id": "c1685dfb6d0b3dcff3b54be485a863fb",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Exploring the Proportional Odds Model for Ordinal Logistic Regression",
      "url": "https://towardsdatascience.com/proportional-odds-model-for-ordinal-logistic-regression/",
      "description": "Understanding and Implementing Brant’s Tests in Ordinal Logistic Regression with Python\nThe post Exploring the Proportional Odds Model for Ordinal Logistic Regression appeared first on Towards Data Science.",
      "published_date": "2025-06-12T05:45:56+00:00",
      "collected_at": "2025-06-13T12:33:45.489181+00:00",
      "author": "JUNIOR JUMBONG",
      "source_priority": 3,
      "score": 94
    },
    {
      "id": "9063e838e64e6d653b3129520f884b40",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Can AI Truly Develop a Memory That Adapts Like Ours?",
      "url": "https://towardsdatascience.com/can-ai-truly-develop-a-memory-that-adapts-like-ours/",
      "description": "Exploring Titans: A new architecture equipping LLMs with human-inspired memory that learns and updates itself during test-time.\nThe post Can AI Truly Develop a Memory That Adapts Like Ours? appeared first on Towards Data Science.",
      "published_date": "2025-06-12T05:32:11+00:00",
      "collected_at": "2025-06-13T12:33:45.489387+00:00",
      "author": "Moulik Gupta",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "b60c248f372be8d09338700bd0b0e697",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Discovering Hierarchical Latent Capabilities of Language Models via\n  Causal Representation Learning",
      "url": "https://arxiv.org/abs/2506.10378",
      "description": "Faithful evaluation of language model capabilities is crucial for deriving\nactionable insights that can inform model development. However, rigorous causal\nevaluations in this domain face significant methodological challenges,\nincluding complex confounding effects and prohibitive computational costs\nassociated with extensive retraining. To tackle these challenges, we propose a\ncausal representation learning framework wherein observed benchmark performance\nis modeled as a linear transformation of",
      "published_date": "2025-06-12T02:07:42+00:00",
      "collected_at": "2025-06-13T12:33:44.461450+00:00",
      "author": "",
      "source_priority": 1,
      "score": 94
    },
    {
      "id": "f6d81dda61f177f4ac4a259da522fa1d",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Model Context Protocol (MCP) Tutorial: Build Your First MCP Server in 6 Steps",
      "url": "https://towardsdatascience.com/model-context-protocol-mcp-tutorial-build-your-first-mcp-server-in-6-steps/",
      "description": "A beginner-friendly tutorial of MCP architecture, with the focus on MCP server components and applications, guiding through the process of building a custom MCP server that enables code-to-diagram.\nThe post Model Context Protocol (MCP) Tutorial: Build Your First MCP Server in 6 Steps appeared first on Towards Data Science.",
      "published_date": "2025-06-11T19:40:45+00:00",
      "collected_at": "2025-06-13T12:33:45.489583+00:00",
      "author": "Destin Gong",
      "source_priority": 3,
      "score": 93
    },
    {
      "id": "68c07722e8ce23d92d28c6f82a57c3be",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Attention, Please! Revisiting Attentive Probing for Masked Image\n  Modeling",
      "url": "https://arxiv.org/abs/2506.10178",
      "description": "As fine-tuning (FT) becomes increasingly impractical at scale, probing is\nemerging as the preferred evaluation protocol for self-supervised learning\n(SSL). Yet, the standard linear probing (LP) fails to adequately reflect the\npotential of models trained with Masked Image Modeling (MIM), due to the\ndistributed nature of patch tokens. This motivates the need for attentive\nprobing, an alternative that uses attention to selectively aggregate\npatch-level features. Despite its growing adoption, attent",
      "published_date": "2025-06-11T17:10:26+00:00",
      "collected_at": "2025-06-13T12:33:44.458791+00:00",
      "author": "",
      "source_priority": 1,
      "score": 94
    },
    {
      "id": "036d3d830f2eae87fdae67f04880d8b8",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Mobile App Development with Python",
      "url": "https://towardsdatascience.com/mobile-app-development-with-python/",
      "description": "Build iOS & Android Apps with Kivy\nThe post Mobile App Development with Python appeared first on Towards Data Science.",
      "published_date": "2025-06-11T16:55:03+00:00",
      "collected_at": "2025-06-13T12:33:45.554237+00:00",
      "author": "Mauro Di Pietro",
      "source_priority": 3,
      "score": 93
    },
    {
      "id": "82ec97867829f52841e6ad037fbb1e07",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Resa: Transparent Reasoning Models via SAEs",
      "url": "https://arxiv.org/abs/2506.09967",
      "description": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified",
      "published_date": "2025-06-11T13:44:01+00:00",
      "collected_at": "2025-06-13T12:33:44.462401+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "732cb140e94759e87081b3ba3b69e428",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal\n  Gaussian Splatting",
      "url": "https://arxiv.org/abs/2506.09952",
      "description": "The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primit",
      "published_date": "2025-06-11T13:23:21+00:00",
      "collected_at": "2025-06-13T12:33:44.459565+00:00",
      "author": "",
      "source_priority": 1,
      "score": 94
    },
    {
      "id": "8647cd9cdb434e35eec9fb9d5d10b3c7",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Audio Spectrogram Transformers Beyond the Lab",
      "url": "https://towardsdatascience.com/audio-spectrogram-transformers-beyond-the-lab/",
      "description": "A recipe for building a portable soundscape monitoring app with AudioMoth, Raspberry Pi, and a decent dose of deep learning.\nThe post Audio Spectrogram Transformers Beyond the Lab appeared first on Towards Data Science.",
      "published_date": "2025-06-10T23:47:29+00:00",
      "collected_at": "2025-06-13T12:33:45.554473+00:00",
      "author": "Maciej Adamiak",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "670434083c4a15bec64ebb9e2cdcc44c",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Automate Models Training: An MLOps Pipeline with Tekton and Buildpacks",
      "url": "https://towardsdatascience.com/automate-models-training-an-mlops-pipeline-with-tekton-and-buildpacks/",
      "description": "A step-by-step guide to containerizing and orchestrating an ML training workflow without the Dockerfile headache, using a lightweight GPT-2 example.\nThe post Automate Models Training: An MLOps Pipeline with Tekton and Buildpacks appeared first on Towards Data Science.",
      "published_date": "2025-06-10T23:37:18+00:00",
      "collected_at": "2025-06-13T12:33:45.554677+00:00",
      "author": "Sylvain Kalache",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "14d58c32a594dab10314ce70e770deaf",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "10,000x Faster Bayesian Inference: Multi-GPU SVI vs. Traditional MCMC",
      "url": "https://towardsdatascience.com/10000x-faster-bayesian-inference-multi-gpu-svi-vs-traditional-mcmc/",
      "description": "Using GPU acceleration to speed up Bayesian Inference from months to minutes... \nThe post 10,000x Faster Bayesian Inference: Multi-GPU SVI vs. Traditional MCMC appeared first on Towards Data Science.",
      "published_date": "2025-06-10T23:29:05+00:00",
      "collected_at": "2025-06-13T12:33:45.554926+00:00",
      "author": "Derek Tran",
      "source_priority": 3,
      "score": 92
    },
    {
      "id": "027d866f96062018e82268cc4000a5e6",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation",
      "url": "https://arxiv.org/abs/2506.09344",
      "description": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilita",
      "published_date": "2025-06-10T22:50:49+00:00",
      "collected_at": "2025-06-13T12:33:44.462899+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "a28c0119414440455448e512f68a2418",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Token Perturbation Guidance for Diffusion Models",
      "url": "https://arxiv.org/abs/2506.10036",
      "description": "Classifier-free guidance (CFG) has become an essential component of modern\ndiffusion models to enhance both generation quality and alignment with input\nconditions. However, CFG requires specific training procedures and is limited\nto conditional generation. To address these limitations, we propose Token\nPerturbation Guidance (TPG), a novel method that applies perturbation matrices\ndirectly to intermediate token representations within the diffusion network.\nTPG employs a norm-preserving shuffling",
      "published_date": "2025-06-10T17:25:46+00:00",
      "collected_at": "2025-06-13T12:33:44.460228+00:00",
      "author": "",
      "source_priority": 1,
      "score": 92
    },
    {
      "id": "50915c04bf618d568192a3eb44ce6fec",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Applications of Density Estimation to Legal Theory",
      "url": "https://towardsdatascience.com/applications-of-density-estimation-to-legal-theory/",
      "description": "A brief analysis using density estimation to compare the two-verdict and three-verdict systems.\nThe post Applications of Density Estimation to Legal Theory appeared first on Towards Data Science.",
      "published_date": "2025-06-10T16:36:24+00:00",
      "collected_at": "2025-06-13T12:33:45.555124+00:00",
      "author": "Jimin Kang",
      "source_priority": 3,
      "score": 91
    },
    {
      "id": "247080e80e8edd61863173b9d9f4ac59",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated\n  Video Streams",
      "url": "https://arxiv.org/abs/2506.08862",
      "description": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams\nis crucial for numerous real-world applications. However, existing methods\nstruggle to jointly address three key challenges: 1) processing uncalibrated\ninputs in real time, 2) accurately modeling dynamic scene evolution, and 3)\nmaintaining long-term stability and computational efficiency. To this end, we\nintroduce StreamSplat, the first fully feed-forward framework that transforms\nuncalibrated video streams of arbitrar",
      "published_date": "2025-06-10T10:52:36+00:00",
      "collected_at": "2025-06-13T12:33:44.459050+00:00",
      "author": "",
      "source_priority": 1,
      "score": 92
    },
    {
      "id": "2cba20a73aaee2108868ab012d464b99",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Mastering SQL Window Functions",
      "url": "https://towardsdatascience.com/mastering-sql-window-functions/",
      "description": "Understand how to use Window Functions to perform calculations without losing details\nThe post Mastering SQL Window Functions appeared first on Towards Data Science.",
      "published_date": "2025-06-10T05:36:15+00:00",
      "collected_at": "2025-06-13T12:33:45.555321+00:00",
      "author": "Eugenia Anello",
      "source_priority": 3,
      "score": 91
    },
    {
      "id": "f85347fb0559744e9a6053beb02e544f",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Exploratory Data Analysis: Gamma Spectroscopy in Python",
      "url": "https://towardsdatascience.com/exploratory-data-analysis-gamma-spectroscopy-in-python/",
      "description": "Let’s observe the matter on the atomic level\nThe post Exploratory Data Analysis: Gamma Spectroscopy in Python appeared first on Towards Data Science.",
      "published_date": "2025-06-10T05:27:05+00:00",
      "collected_at": "2025-06-13T12:33:45.555526+00:00",
      "author": "Dmitrii Eliuseev",
      "source_priority": 3,
      "score": 91
    },
    {
      "id": "b5e500a2d2ad993b7c2a8d260dbc2196",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "A Bird’s-Eye View of Linear Algebra: Measure of a Map — Determinants",
      "url": "https://towardsdatascience.com/a-birds-eye-view-of-linear-algebra-measure-of-a-map-determinants/",
      "description": "We roll up our sleeves and start to deal with matrices\nThe post A Bird’s-Eye View of Linear Algebra: Measure of a Map — Determinants appeared first on Towards Data Science.",
      "published_date": "2025-06-10T05:00:27+00:00",
      "collected_at": "2025-06-13T12:33:45.555727+00:00",
      "author": "Rohit Pandey",
      "source_priority": 3,
      "score": 91
    },
    {
      "id": "8974cbf315fc128cbdcbf27fffa994ba",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "How to Transition From Data Analyst to Data Scientist",
      "url": "https://towardsdatascience.com/how-to-transition-from-data-analyst-to-data-scientist/",
      "description": "Playbook on how data analysts can become data scientists\nThe post How to Transition From Data Analyst to Data Scientist appeared first on Towards Data Science.",
      "published_date": "2025-06-09T23:09:55+00:00",
      "collected_at": "2025-06-13T12:33:45.556138+00:00",
      "author": "Egor Howell",
      "source_priority": 3,
      "score": 90
    },
    {
      "id": "5bb2aa15fbc6f8e0fd06eba34a4606b2",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Trying to Stay Sane in the Age of AI",
      "url": "https://towardsdatascience.com/trying-to-stay-sane-in-the-age-of-ai/",
      "description": "A machine learning engineer’s quiet way of not losing her mind\nThe post Trying to Stay Sane in the Age of AI appeared first on Towards Data Science.",
      "published_date": "2025-06-09T22:50:40+00:00",
      "collected_at": "2025-06-13T12:33:45.556336+00:00",
      "author": "Amy Ma",
      "source_priority": 3,
      "score": 100
    },
    {
      "id": "93f37a4ba94dd7229d84452a6441c9f6",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and\n  Future Directions",
      "url": "https://arxiv.org/abs/2506.08234",
      "description": "Recent advancements in large language models (LLMs) and AI systems have led\nto a paradigm shift in the design and optimization of complex AI workflows. By\nintegrating multiple components, compound AI systems have become increasingly\nadept at performing sophisticated tasks. However, as these systems grow in\ncomplexity, new challenges arise in optimizing not only individual components\nbut also their interactions. While traditional optimization methods such as\nsupervised fine-tuning (SFT) and reinf",
      "published_date": "2025-06-09T17:04:14+00:00",
      "collected_at": "2025-06-13T12:33:44.459306+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "bff69df57dc22868213189637a98e9ba",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "LLM Unlearning Should Be Form-Independent",
      "url": "https://arxiv.org/abs/2506.07795",
      "description": "Large Language Model (LLM) unlearning aims to erase or suppress undesirable\nknowledge within the model, offering promise for controlling harmful or private\ninformation to prevent misuse. However, recent studies highlight its limited\nefficacy in real-world scenarios, hindering practical adoption. In this study,\nwe identify a pervasive issue underlying many downstream failures: the\neffectiveness of existing unlearning methods heavily depends on the form of\ntraining samples and frequently fails to",
      "published_date": "2025-06-09T10:21:25+00:00",
      "collected_at": "2025-06-13T12:33:44.459952+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "cf3130e8fb5e3059bbebaf5776dcd7b8",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "What Makes a Good Natural Language Prompt?",
      "url": "https://arxiv.org/abs/2506.06950",
      "description": "As large language models (LLMs) have progressed towards more human-like and\nhuman--AI communications have become prevalent, prompting has emerged as a\ndecisive component. However, there is limited conceptual consensus on what\nexactly quantifies natural language prompts. We attempt to address this\nquestion by conducting a meta-analysis surveying more than 150\nprompting-related papers from leading NLP and AI conferences from 2022 to 2025\nand blogs. We propose a property- and human-centric framewor",
      "published_date": "2025-06-07T19:19:27+00:00",
      "collected_at": "2025-06-13T12:33:44.461936+00:00",
      "author": "",
      "source_priority": 1,
      "score": 100
    },
    {
      "id": "e6b9bdd5b896ba5256327cd90e65f9bb",
      "source_id": "huggingface_papers_api",
      "source": "huggingface_papers_api",
      "category": "Open Source",
      "title": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation\n  Models via Generative Continual Learning",
      "url": "https://arxiv.org/abs/2506.06694",
      "description": "Foundation models have revolutionized fields such as natural language\nprocessing and computer vision by enabling general-purpose learning across\ndiverse tasks and datasets. However, building analogous models for human\nmobility remains challenging due to the privacy-sensitive nature of mobility\ndata and the resulting data silos across institutions. To bridge this gap, we\npropose MoveGCL, a scalable and privacy-preserving framework for training\nmobility foundation models via generative continual l",
      "published_date": "2025-06-07T03:19:11+00:00",
      "collected_at": "2025-06-13T12:33:44.462169+00:00",
      "author": "",
      "source_priority": 1,
      "score": 88
    },
    {
      "id": "ba0a1d0ca488df04266d6f48441d1523",
      "source_id": "pytorch_blog",
      "source": "Blog – PyTorch",
      "category": "Open Source",
      "title": "HuggingFace Safetensors Support in PyTorch Distributed Checkpointing",
      "url": "https://pytorch.org/blog/huggingface-safetensors-support-in-pytorch-distributed-checkpointing/",
      "description": "Summary  PyTorch Distributed Checkpointing (DCP) is making investments into addressing the interoperability blockers to ensure that popular formats, like HuggingFace safetensors, can work well with PyTorch’s ecosystem. Since HuggingFace has...",
      "published_date": "2025-06-06T19:17:46+00:00",
      "collected_at": "2025-06-13T12:33:44.678015+00:00",
      "author": "Ankita George, Saurabh Mishra, Joe Cummings, Philip Bontrager, Daulet Askarov, Teja Rao, Chien-Chin Huang, Ela Krepska, Jafar Taghiyar",
      "source_priority": 2,
      "score": 88
    },
    {
      "id": "f2e2ef796ae85073f0397c059bcd40ea",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Not Everything Needs Automation: 5 Practical AI Agents That Deliver Enterprise Value",
      "url": "https://towardsdatascience.com/not-everything-needs-automation-5-practical-ai-agents-that-deliver-enterprise-value/",
      "description": "What actually works with AI agents inside enterprise organizations?\nThe post Not Everything Needs Automation: 5 Practical AI Agents That Deliver Enterprise Value appeared first on Towards Data Science.",
      "published_date": "2025-06-06T17:54:12+00:00",
      "collected_at": "2025-06-13T12:33:45.556529+00:00",
      "author": "Weiwei Hu",
      "source_priority": 3,
      "score": 87
    },
    {
      "id": "3d5ee6adb52013963931f8bccef6033c",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "Prescriptive Modeling Unpacked: A Complete Guide to Intervention With Bayesian Modeling.",
      "url": "https://towardsdatascience.com/prescriptive-modeling-unpacked-a-complete-guide-to-intervention-with-bayesian-modeling/",
      "description": "Learn how to move beyond prediction and actively make intervention through prescriptive modeling. This in-depth guide walks you through Bayesian approaches to system intervention, with practical examples in predictive maintenance.\nThe post Prescriptive Modeling Unpacked: A Complete Guide to Intervention With Bayesian Modeling. appeared first on Towards Data Science.",
      "published_date": "2025-06-06T17:23:32+00:00",
      "collected_at": "2025-06-13T12:33:45.556718+00:00",
      "author": "Erdogan Taskesen",
      "source_priority": 3,
      "score": 87
    },
    {
      "id": "1baeabdb760d5d46d8825a522a214e3e",
      "source_id": "towards_data_science",
      "source": "Towards Data Science",
      "category": "Open Source",
      "title": "How I Automated My Machine Learning Workflow with Just 10 Lines of Python",
      "url": "https://towardsdatascience.com/how-i-automated-my-machine-learning-workflow-with-just-10-lines-of-python/",
      "description": "Use LazyPredict and PyCaret to skip the grunt work and jump straight to performance.\nThe post How I Automated My Machine Learning Workflow with Just 10 Lines of Python appeared first on Towards Data Science.",
      "published_date": "2025-06-06T13:11:46+00:00",
      "collected_at": "2025-06-13T12:33:45.556933+00:00",
      "author": "Himanshu Sharma",
      "source_priority": 3,
      "score": 98
    }
  ],
  "updated": "2025-06-13T11:49:07.179140"
}