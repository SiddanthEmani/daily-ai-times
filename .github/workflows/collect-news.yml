name: Daily AI Times - Modern Serverless News Pipeline

on:
  schedule:
    - cron: '0 */4 * * *'  # Every 4 hours - Fresh news cycle
  workflow_dispatch:
    inputs:
      sources:
        description: 'Sources to collect (comma-separated or "all")'
        required: false
        default: 'all'
      force_refresh:
        description: 'Force refresh'
        required: false
        type: boolean
        default: false
      debug:
        description: 'Enable debug mode (show detailed deployment structure)'
        required: false
        type: boolean
        default: false
      skip_deploy:
        description: 'Skip deployment (useful for testing)'
        required: false
        type: boolean
        default: false

permissions:
  contents: write
  pages: write
  id-token: write

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  PYTHONUNBUFFERED: 1
  PYTHONIOENCODING: utf-8

jobs:
  # ================================================================
  # JOB 1: AI NEWS PROCESSING - Complete Pipeline with Orchestrator
  # OPTIMIZED: Multi-layer caching reduces dependency installation from 4min → 30-60sec
  # ================================================================
  ai-processing:
    name: 🤖 AI News Processing Pipeline
    runs-on: ubuntu-latest
    timeout-minutes: 45
    concurrency:
      group: news-processing
      cancel-in-progress: false
    
    outputs:
      has_articles: ${{ steps.process.outputs.has_articles }}
      article_count: ${{ steps.process.outputs.article_count }}
      pipeline_version: ${{ steps.process.outputs.pipeline_version }}
      processing_time: ${{ steps.process.outputs.processing_time }}
      
    steps:
    - name: 📦 Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
    
    - name: 🐍 Set up Python environment
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: 🗄️ Cache system packages
      uses: actions/cache@v4
      with:
        path: /var/cache/apt
        key: apt-cache-${{ runner.os }}-${{ hashFiles('.github/workflows/collect-news.yml') }}
        restore-keys: |
          apt-cache-${{ runner.os }}-
    
    - name: 🗄️ Cache Python dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          ~/.local/lib/python${{ env.PYTHON_VERSION }}/site-packages
          /opt/hostedtoolcache/Python/${{ env.PYTHON_VERSION }}/*/lib/python${{ env.PYTHON_VERSION }}/site-packages
        key: python-deps-${{ runner.os }}-${{ env.PYTHON_VERSION }}-${{ hashFiles('src/backend/requirements.txt') }}
        restore-keys: |
          python-deps-${{ runner.os }}-${{ env.PYTHON_VERSION }}-
    
    - name: 📚 Install dependencies (Optimized)
      run: |
        # System packages (use cache when available)
        if [ ! -f /var/cache/apt/pkgcache.bin ] || [ $(find /var/cache/apt -name "*.deb" | wc -l) -lt 50 ]; then
          echo "📦 Installing system packages..."
          sudo apt-get update -qq > /dev/null 2>&1
        else
          echo "✅ Using cached system packages"
        fi
        sudo apt-get install -y --no-install-recommends jq curl
        
        # Python packages (optimized installation)
        echo "🐍 Installing Python dependencies..."
        pip install --upgrade pip setuptools wheel
        pip install --cache-dir ~/.cache/pip --only-binary=:all: \
          -r src/backend/requirements.txt || \
        pip install --cache-dir ~/.cache/pip \
          -r src/backend/requirements.txt
    
    - name: 🚀 Run AI News Processing Orchestrator
      id: process
      env:
        GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      run: |
        echo "🚀 Starting modern AI news processing pipeline..."
        
        # Ensure required directories exist
        mkdir -p src/backend/api src/frontend/api test_output
        
        # Ensure orchestrator script is executable
        chmod +x ./orchestrator.run
        
        # Run the new orchestrator with proper timeout and error handling
        if timeout 40m ./orchestrator.run; then
          echo "✅ AI processing pipeline completed successfully"
          
          # Validate outputs exist
          if [ -f "src/frontend/api/latest.json" ] && ls test_output/orchestrator_results_*.json >/dev/null 2>&1; then
            # Extract pipeline metrics
            article_count=$(jq '.articles | length // 0' src/frontend/api/latest.json)
            pipeline_version=$(jq -r '.pipeline_info.version // "unknown"' src/frontend/api/latest.json)
            generated_at=$(jq -r '.generated_at' src/frontend/api/latest.json)
            
            # Get processing time from orchestrator results
            results_file=$(ls test_output/orchestrator_results_*.json | head -1)
            processing_time=$(jq '.results_summary.processing_time_seconds // 0' "$results_file")
            
            echo "has_articles=true" >> $GITHUB_OUTPUT
            echo "article_count=$article_count" >> $GITHUB_OUTPUT
            echo "pipeline_version=$pipeline_version" >> $GITHUB_OUTPUT
            echo "processing_time=$processing_time" >> $GITHUB_OUTPUT
            
            echo "✅ Processed $article_count articles with $pipeline_version"
            echo "📅 Generated at: $generated_at"
            echo "⏱️ Processing time: ${processing_time}s"
          else
            echo "❌ Missing required output files"
            ls -la src/frontend/api/ test_output/ || true
            exit 1
          fi
        else
          echo "❌ AI processing pipeline failed"
          exit 1
        fi
    
    - name: 💾 Commit processed content
      id: commit
      run: |
        git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        # Remove sensitive usage reports from commit
        rm -rf test_output/groq_* logs/ || true
        echo "🔒 Removed private usage reports from commit (available in artifacts)"
        
        # Add API files, processed data, and generated audio
        git add src/backend/api/ src/frontend/api/ src/frontend/assets/audio/
        
        if git diff --staged --quiet; then
          echo "ℹ️ No new content changes to commit"
        else
          article_count="${{ steps.process.outputs.article_count }}"
          pipeline_version="${{ steps.process.outputs.pipeline_version }}"
          processing_time="${{ steps.process.outputs.processing_time }}"
          
          commit_msg="🤖 AI Pipeline Update: $article_count articles ($pipeline_version) - $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          
          git commit -m "$commit_msg"
          git push
          
          echo "✅ AI-processed content committed and pushed"
          echo "🌐 Fresh content ready for deployment"
        fi
    
    - name: 📤 Upload processed content
      uses: actions/upload-artifact@v4
      with:
        name: processed-content
        path: |
          src/backend/api/
          src/frontend/api/
          src/frontend/assets/audio/
          test_output/orchestrator_results_*.json
        retention-days: 7

    - name: 🔒 Upload usage reports (Private)
      uses: actions/upload-artifact@v4
      with:
        name: usage-reports
        path: |
          test_output/groq_*
          test_output/*usage*
          test_output/cost_*
        retention-days: 30
      if: always()

  # ================================================================
  # JOB 2: FRONTEND DEPLOYMENT - Deploy to GitHub Pages
  # ================================================================
  deploy-frontend:
    name: 🌐 Deploy Fresh Content to GitHub Pages
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: ai-processing
    if: success() && (needs.ai-processing.outputs.has_articles == 'true' || github.event.inputs.force_refresh == 'true') && github.event.inputs.skip_deploy != 'true'
    
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
    - name: 📦 Checkout repository
      uses: actions/checkout@v4
      with:
        ref: ${{ github.ref }}
        
    - name: 📥 Download processed content
      uses: actions/download-artifact@v4
      with:
        name: processed-content
        path: processed-content/
        
    - name: 🏗️ Prepare deployment site
      run: |
        echo "🏗️ Preparing fresh news content for deployment..."
        
        # Create clean deployment directory
        mkdir -p site/
        
        # Copy frontend files
        echo "📋 Copying frontend assets..."
        cp -r src/frontend/* site/
        
        # Integrate fresh API content and audio
        echo "🔄 Integrating fresh API content and audio..."
        if [ -d "processed-content/src/frontend/api" ]; then
          cp -r processed-content/src/frontend/api/* site/api/ 2>/dev/null || true
        fi
        if [ -d "processed-content/src/backend/api" ]; then
          cp -r processed-content/src/backend/api/* site/api/ 2>/dev/null || true
        fi
        if [ -d "processed-content/src/frontend/assets/audio" ]; then
          echo "🎵 Copying fresh audio content..."
          mkdir -p site/assets/audio/
          cp -r processed-content/src/frontend/assets/audio/* site/assets/audio/ 2>/dev/null || true
        fi
        
        # Configure analytics if available
        if [ -f "site/index.html" ]; then
          if [ -n "${{ secrets.GOOGLE_ANALYTICS_ID }}" ]; then
            echo "📊 Configuring Google Analytics with ID: ${{ secrets.GOOGLE_ANALYTICS_ID }}"
            sed -i "s|__GOOGLE_ANALYTICS__|${{ secrets.GOOGLE_ANALYTICS_ID }}|g" site/index.html
            echo "✅ Google Analytics configured successfully"
          else
            echo "⚠️ GOOGLE_ANALYTICS_ID secret not set - Google Analytics will be disabled"
            # Remove the entire Google Analytics block when no ID is provided
            sed -i '/<!-- __GOOGLE_ANALYTICS__ -->/,/<!-- __GOOGLE_ANALYTICS__ -->/d' site/index.html
            echo "🧹 Google Analytics block removed (no secret configured)"
          fi
        fi
        
        echo "✅ Deployment site prepared"

    - name: 🔍 Validate deployment
      run: |
        echo "🔍 Validating deployment content..."
        
        # Check required files
        if [ ! -f "site/index.html" ]; then
          echo "❌ Missing index.html"
          exit 1
        fi
        
        # Validate API content
        if [ -f "site/api/latest.json" ]; then
          article_count=$(jq '.articles | length // 0' site/api/latest.json)
          generated_at=$(jq -r '.generated_at // "unknown"' site/api/latest.json)
          echo "✅ Fresh content: $article_count articles (generated: $generated_at)"
        else
          echo "⚠️ No latest.json found - checking for fallback content"
        fi
        
        # Validate audio content
        if [ -f "site/assets/audio/latest-podcast.wav" ]; then
          audio_size=$(stat -c%s "site/assets/audio/latest-podcast.wav" 2>/dev/null || echo "0")
          echo "✅ Fresh audio: latest-podcast.wav (${audio_size} bytes)"
        else
          echo "⚠️ No fresh audio found - users may hear old podcast"
        fi
        
        total_files=$(find site -type f | wc -l)
        echo "✅ Deployment validated: $total_files files ready"
        
        if [ "${{ github.event.inputs.debug }}" = "true" ]; then
          echo "=== Debug: Deployment structure ==="
          find site -type f | sort
        fi

    - name: 📤 Upload Pages artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: site
    
    - name: 🚀 Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4
      with:
        timeout: 600000

  # ================================================================
  # JOB 3: WORKFLOW MONITORING & SUMMARY
  # ================================================================
  summary:
    name: 📊 Pipeline Summary
    runs-on: ubuntu-latest
    needs: [ai-processing, deploy-frontend]
    if: always()
    
    steps:
    - name: 📋 Generate pipeline summary
      run: |
        echo "# 🤖 Daily AI Times - Modern AI Processing Pipeline" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "🚀 **Modern Architecture**: Single orchestrator handles collection → AI processing → API generation" >> $GITHUB_STEP_SUMMARY
        echo "⚡ **Performance**: Advanced multi-agent AI processing with consensus algorithms" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "## 🔧 Configuration" >> $GITHUB_STEP_SUMMARY
        echo "- **Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Sources:** ${{ github.event.inputs.sources || 'all' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Force Refresh:** ${{ github.event.inputs.force_refresh || 'false' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Debug Mode:** ${{ github.event.inputs.debug || 'false' }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "## 📊 Pipeline Results" >> $GITHUB_STEP_SUMMARY
        
        # AI Processing Results
        if [ "${{ needs.ai-processing.result }}" = "success" ]; then
          echo "✅ **AI Processing:** ${{ needs.ai-processing.outputs.article_count }} articles processed" >> $GITHUB_STEP_SUMMARY
          echo "🔄 **Pipeline Version:** ${{ needs.ai-processing.outputs.pipeline_version }}" >> $GITHUB_STEP_SUMMARY
          echo "⏱️ **Processing Time:** ${{ needs.ai-processing.outputs.processing_time }}s" >> $GITHUB_STEP_SUMMARY
          echo "🤖 **AI Features:** Multi-agent consensus, deep intelligence analysis, content classification" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **AI Processing:** Failed - check processing logs" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Deployment Results
        if [ "${{ needs.deploy-frontend.result }}" = "success" ]; then
          echo "✅ **Deployment:** Successfully updated GitHub Pages" >> $GITHUB_STEP_SUMMARY
          echo "🌐 **User Access:** Fresh AI-curated content available immediately" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ needs.deploy-frontend.result }}" = "skipped" ]; then
          echo "⏭️ **Deployment:** Skipped (no new content or disabled)" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **Deployment:** Failed - users may see stale content" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## 🏗️ Modern Architecture Benefits" >> $GITHUB_STEP_SUMMARY
        echo "- **Unified Pipeline:** Single orchestrator replaces complex multi-job workflow" >> $GITHUB_STEP_SUMMARY
        echo "- **Advanced AI:** Multi-agent swarm intelligence with consensus algorithms" >> $GITHUB_STEP_SUMMARY
        echo "- **Optimized Performance:** Multi-layer caching reduces setup time by 70-85%" >> $GITHUB_STEP_SUMMARY
        echo "- **Robust Error Handling:** Built-in timeouts, retries, and graceful degradation" >> $GITHUB_STEP_SUMMARY
        echo "- **Enhanced Monitoring:** Detailed pipeline metrics and usage tracking" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.ai-processing.result }}" != "success" ] || [ "${{ needs.deploy-frontend.result }}" = "failure" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## 🚨 Troubleshooting" >> $GITHUB_STEP_SUMMARY
          echo "Check individual job logs for detailed error information:" >> $GITHUB_STEP_SUMMARY
          echo "- AI processing issues: Verify GROQ_API_KEY and rate limits" >> $GITHUB_STEP_SUMMARY
          echo "- Deployment issues: Ensure GitHub Pages is properly configured" >> $GITHUB_STEP_SUMMARY
        fi 