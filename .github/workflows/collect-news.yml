name: NewsXP AI - Modern Serverless News Pipeline

on:
  schedule:
    - cron: '0 */4 * * *'  # Every 4 hours - Fresh news cycle
  workflow_dispatch:
    inputs:
      sources:
        description: 'Sources to collect (comma-separated or "all")'
        required: false
        default: 'all'
      force_refresh:
        description: 'Force refresh'
        required: false
        type: boolean
        default: false
      debug:
        description: 'Enable debug mode (show detailed deployment structure)'
        required: false
        type: boolean
        default: false
      skip_deploy:
        description: 'Skip deployment (useful for testing)'
        required: false
        type: boolean
        default: false

permissions:
  contents: write
  pages: write
  id-token: write

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  PYTHONUNBUFFERED: 1
  PYTHONIOENCODING: utf-8

jobs:
  # ================================================================
  # JOB 1: AI NEWS PROCESSING - Complete Pipeline with Orchestrator
  # ================================================================
  ai-processing:
    name: ðŸ¤– AI News Processing Pipeline
    runs-on: ubuntu-latest
    timeout-minutes: 45
    concurrency:
      group: news-processing
      cancel-in-progress: false
    
    outputs:
      has_articles: ${{ steps.process.outputs.has_articles }}
      article_count: ${{ steps.process.outputs.article_count }}
      pipeline_version: ${{ steps.process.outputs.pipeline_version }}
      processing_time: ${{ steps.process.outputs.processing_time }}
      
    steps:
    - name: ðŸ“¦ Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
    
    - name: ðŸ Set up Python environment
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: src/backend/requirements.txt
    
    - name: ðŸ“š Install dependencies
      run: |
        sudo apt-get update -qq && sudo apt-get install -y jq curl
        pip install --upgrade pip
        pip install -r src/backend/requirements.txt
    
    - name: ðŸš€ Run AI News Processing Orchestrator
      id: process
      env:
        GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
      run: |
        echo "ðŸš€ Starting modern AI news processing pipeline..."
        
        # Ensure required directories exist
        mkdir -p src/backend/api src/frontend/api test_output
        
        # Run the new orchestrator with proper timeout and error handling
        if timeout 40m ./orchestrator.run --generate-api; then
          echo "âœ… AI processing pipeline completed successfully"
          
          # Validate outputs exist
          if [ -f "src/frontend/api/latest.json" ] && [ -f "test_output/orchestrator_results_"*.json ]; then
            # Extract pipeline metrics
            article_count=$(jq '.articles | length // 0' src/frontend/api/latest.json)
            pipeline_version=$(jq -r '.pipeline_info.version // "unknown"' src/frontend/api/latest.json)
            generated_at=$(jq -r '.generated_at' src/frontend/api/latest.json)
            
            # Get processing time from orchestrator results
            results_file=$(ls test_output/orchestrator_results_*.json | head -1)
            processing_time=$(jq '.results_summary.processing_time_seconds // 0' "$results_file")
            
            echo "has_articles=true" >> $GITHUB_OUTPUT
            echo "article_count=$article_count" >> $GITHUB_OUTPUT
            echo "pipeline_version=$pipeline_version" >> $GITHUB_OUTPUT
            echo "processing_time=$processing_time" >> $GITHUB_OUTPUT
            
            echo "âœ… Processed $article_count articles with $pipeline_version"
            echo "ðŸ“… Generated at: $generated_at"
            echo "â±ï¸ Processing time: ${processing_time}s"
          else
            echo "âŒ Missing required output files"
            ls -la src/frontend/api/ test_output/ || true
            exit 1
          fi
        else
          echo "âŒ AI processing pipeline failed"
          exit 1
        fi
    
    - name: ðŸ’¾ Commit processed content
      id: commit
      run: |
        git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        # Remove sensitive usage reports from commit
        rm -rf test_output/groq_* logs/ || true
        echo "ðŸ”’ Removed private usage reports from commit (available in artifacts)"
        
        # Add API files and processed data
        git add src/backend/api/ src/frontend/api/
        
        if git diff --staged --quiet; then
          echo "â„¹ï¸ No new content changes to commit"
        else
          article_count="${{ steps.process.outputs.article_count }}"
          pipeline_version="${{ steps.process.outputs.pipeline_version }}"
          processing_time="${{ steps.process.outputs.processing_time }}"
          
          commit_msg="ðŸ¤– AI Pipeline Update: $article_count articles ($pipeline_version) - $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          
          git commit -m "$commit_msg"
          git push
          
          echo "âœ… AI-processed content committed and pushed"
          echo "ðŸŒ Fresh content ready for deployment"
        fi
    
    - name: ðŸ“¤ Upload processed content
      uses: actions/upload-artifact@v4
      with:
        name: processed-content
        path: |
          src/backend/api/
          src/frontend/api/
          test_output/orchestrator_results_*.json
        retention-days: 7

    - name: ðŸ”’ Upload usage reports (Private)
      uses: actions/upload-artifact@v4
      with:
        name: usage-reports
        path: |
          test_output/groq_*
          test_output/*usage*
          test_output/cost_*
        retention-days: 30
      if: always()

  # ================================================================
  # JOB 2: FRONTEND DEPLOYMENT - Deploy to GitHub Pages
  # ================================================================
  deploy-frontend:
    name: ðŸŒ Deploy Fresh Content to GitHub Pages
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: ai-processing
    if: success() && (needs.ai-processing.outputs.has_articles == 'true' || github.event.inputs.force_refresh == 'true') && github.event.inputs.skip_deploy != 'true'
    
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
    - name: ðŸ“¦ Checkout repository
      uses: actions/checkout@v4
      with:
        ref: ${{ github.ref }}
        
    - name: ðŸ“¥ Download processed content
      uses: actions/download-artifact@v4
      with:
        name: processed-content
        path: processed-content/
        
    - name: ðŸ—ï¸ Prepare deployment site
      run: |
        echo "ðŸ—ï¸ Preparing fresh news content for deployment..."
        
        # Create clean deployment directory
        mkdir -p site/
        
        # Copy frontend files
        echo "ðŸ“‹ Copying frontend assets..."
        cp -r src/frontend/* site/
        
        # Integrate fresh API content
        echo "ðŸ”„ Integrating fresh API content..."
        if [ -d "processed-content/src/frontend/api" ]; then
          cp -r processed-content/src/frontend/api/* site/api/ 2>/dev/null || true
        fi
        if [ -d "processed-content/src/backend/api" ]; then
          cp -r processed-content/src/backend/api/* site/api/ 2>/dev/null || true
        fi
        
        # Configure analytics if available
        if [ -f "site/index.html" ]; then
          if [ -n "${{ secrets.GOOGLE_ANALYTICS_ID }}" ]; then
            echo "ðŸ“Š Configuring Google Analytics..."
            sed -i "s|__GOOGLE_ANALYTICS__|${{ secrets.GOOGLE_ANALYTICS_ID }}|g" site/index.html
          else
            echo "ðŸ§¹ Removing analytics placeholder..."
            sed -i 's|__GOOGLE_ANALYTICS__||g' site/index.html
          fi
        fi
        
        echo "âœ… Deployment site prepared"

    - name: ðŸ” Validate deployment
      run: |
        echo "ðŸ” Validating deployment content..."
        
        # Check required files
        if [ ! -f "site/index.html" ]; then
          echo "âŒ Missing index.html"
          exit 1
        fi
        
        # Validate API content
        if [ -f "site/api/latest.json" ]; then
          article_count=$(jq '.articles | length // 0' site/api/latest.json)
          generated_at=$(jq -r '.generated_at // "unknown"' site/api/latest.json)
          echo "âœ… Fresh content: $article_count articles (generated: $generated_at)"
        else
          echo "âš ï¸ No latest.json found - checking for fallback content"
        fi
        
        total_files=$(find site -type f | wc -l)
        echo "âœ… Deployment validated: $total_files files ready"
        
        if [ "${{ github.event.inputs.debug }}" = "true" ]; then
          echo "=== Debug: Deployment structure ==="
          find site -type f | sort
        fi

    - name: ðŸ“¤ Upload Pages artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: site
    
    - name: ðŸš€ Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4
      with:
        timeout: 600000

  # ================================================================
  # JOB 3: WORKFLOW MONITORING & SUMMARY
  # ================================================================
  summary:
    name: ðŸ“Š Pipeline Summary
    runs-on: ubuntu-latest
    needs: [ai-processing, deploy-frontend]
    if: always()
    
    steps:
    - name: ðŸ“‹ Generate pipeline summary
      run: |
        echo "# ðŸ¤– NewsXP AI - Modern AI Processing Pipeline" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "ðŸš€ **Modern Architecture**: Single orchestrator handles collection â†’ AI processing â†’ API generation" >> $GITHUB_STEP_SUMMARY
        echo "âš¡ **Performance**: Advanced multi-agent AI processing with consensus algorithms" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "## ðŸ”§ Configuration" >> $GITHUB_STEP_SUMMARY
        echo "- **Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Sources:** ${{ github.event.inputs.sources || 'all' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Force Refresh:** ${{ github.event.inputs.force_refresh || 'false' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Debug Mode:** ${{ github.event.inputs.debug || 'false' }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "## ðŸ“Š Pipeline Results" >> $GITHUB_STEP_SUMMARY
        
        # AI Processing Results
        if [ "${{ needs.ai-processing.result }}" = "success" ]; then
          echo "âœ… **AI Processing:** ${{ needs.ai-processing.outputs.article_count }} articles processed" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ”„ **Pipeline Version:** ${{ needs.ai-processing.outputs.pipeline_version }}" >> $GITHUB_STEP_SUMMARY
          echo "â±ï¸ **Processing Time:** ${{ needs.ai-processing.outputs.processing_time }}s" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ¤– **AI Features:** Multi-agent consensus, deep intelligence analysis, content classification" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **AI Processing:** Failed - check processing logs" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Deployment Results
        if [ "${{ needs.deploy-frontend.result }}" = "success" ]; then
          echo "âœ… **Deployment:** Successfully updated GitHub Pages" >> $GITHUB_STEP_SUMMARY
          echo "ðŸŒ **User Access:** Fresh AI-curated content available immediately" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ needs.deploy-frontend.result }}" = "skipped" ]; then
          echo "â­ï¸ **Deployment:** Skipped (no new content or disabled)" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Deployment:** Failed - users may see stale content" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## ðŸ—ï¸ Modern Architecture Benefits" >> $GITHUB_STEP_SUMMARY
        echo "- **Unified Pipeline:** Single orchestrator replaces complex multi-job workflow" >> $GITHUB_STEP_SUMMARY
        echo "- **Advanced AI:** Multi-agent swarm intelligence with consensus algorithms" >> $GITHUB_STEP_SUMMARY
        echo "- **Robust Error Handling:** Built-in timeouts, retries, and graceful degradation" >> $GITHUB_STEP_SUMMARY
        echo "- **Better Performance:** Optimized processing with distributed agent workloads" >> $GITHUB_STEP_SUMMARY
        echo "- **Enhanced Monitoring:** Detailed pipeline metrics and usage tracking" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.ai-processing.result }}" != "success" ] || [ "${{ needs.deploy-frontend.result }}" = "failure" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ðŸš¨ Troubleshooting" >> $GITHUB_STEP_SUMMARY
          echo "Check individual job logs for detailed error information:" >> $GITHUB_STEP_SUMMARY
          echo "- AI processing issues: Verify GROQ_API_KEY and rate limits" >> $GITHUB_STEP_SUMMARY
          echo "- Deployment issues: Ensure GitHub Pages is properly configured" >> $GITHUB_STEP_SUMMARY
        fi 