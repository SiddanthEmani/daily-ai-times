name: NewsXP AI - Serverless News Pipeline

on:
  schedule:
    - cron: '0 */4 * * *'  # Every 4 hours - Fresh news cycle
  workflow_dispatch:
    inputs:
      sources:
        description: 'Sources to collect (comma-separated or "all")'
        required: false
        default: 'all'
      force_refresh:
        description: 'Force refresh'
        required: false
        type: boolean
        default: false
      debug:
        description: 'Enable debug mode (show detailed deployment structure)'
        required: false
        type: boolean
        default: false
      skip_deploy:
        description: 'Skip deployment (useful for testing)'
        required: false
        type: boolean
        default: false

permissions:
  contents: write
  pages: write
  id-token: write

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  PYTHONUNBUFFERED: 1
  PYTHONIOENCODING: utf-8

jobs:
  # ================================================================
  # JOB 1: NEWS COLLECTION - Collect Raw Articles
  # ================================================================
  collect-news:
    name: ðŸ“° Collect Raw News Articles
    runs-on: ubuntu-latest
    timeout-minutes: 10
    concurrency:
      group: news-collection
      cancel-in-progress: false
    
    outputs:
      has_articles: ${{ steps.collect.outputs.has_articles }}
      article_count: ${{ steps.collect.outputs.article_count }}
      
    steps:
    - name: ðŸ“¦ Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
    
    - name: ðŸ Set up Python environment
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: src/backend/requirements.txt
    
    - name: ðŸ“š Install dependencies
      run: |
        sudo apt-get update -qq && sudo apt-get install -y jq curl
        pip install --upgrade pip
        pip install -r src/backend/requirements.txt
    
    - name: ï¿½ Collect raw news articles
      id: collect
      run: |
        echo "ðŸš€ Starting news collection cycle..."
        max_attempts=3
        attempt=1
        
        while [ $attempt -le $max_attempts ]; do
          echo "ðŸ“Š Collection attempt $attempt/$max_attempts"
          
          # Create test output directory for pipeline artifacts
          mkdir -p test_output
          
          # Run the news collection with proper output path and config
          if python src/backend/collectors/collect_news.py --output test_output/collected_articles.json --config src/shared/config/sources.json --verbose; then
            echo "âœ… News collection successful"
            
            if [ -f "test_output/collected_articles.json" ]; then
              # Create backend data directory structure
              mkdir -p src/backend/data
              cp test_output/collected_articles.json src/backend/data/news.json
              
              # Count collected articles
              article_count=$(jq '.articles | length // 0' test_output/collected_articles.json)
              echo "has_articles=true" >> $GITHUB_OUTPUT
              echo "article_count=$article_count" >> $GITHUB_OUTPUT
              echo "âœ… Collected $article_count raw articles for processing"
            else
              echo "âŒ Collection completed but missing results"
              exit 1
            fi
            break
          else
            echo "âŒ Collection attempt $attempt failed"
            if [ $attempt -eq $max_attempts ]; then
              echo "ðŸš¨ All collection attempts failed"
              exit 1
            fi
            sleep 30
            ((attempt++))
          fi
        done
    
    - name: ðŸ“¤ Upload collected articles
      uses: actions/upload-artifact@v4
      with:
        name: collected-articles
        path: |
          test_output/collected_articles.json
          src/backend/data/news.json
        retention-days: 1

  # ================================================================
  # JOB 2: GROQ AI PROCESSING - Process Articles with AI
  # ================================================================
  groq-processing:
    name: ðŸ¤– Groq AI Processing Pipeline
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: collect-news
    if: success() && needs.collect-news.outputs.has_articles == 'true'
    concurrency:
      group: groq-processing
      cancel-in-progress: false
    
    outputs:
      has_changes: ${{ steps.commit.outputs.has_changes }}
      article_count: ${{ steps.verify.outputs.article_count }}
      pipeline_version: ${{ steps.verify.outputs.pipeline_version }}
      
    steps:
    - name: ðŸ“¦ Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
    
    - name: ðŸ Set up Python environment
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: src/backend/requirements.txt
    
    - name: ðŸ“š Install dependencies
      run: |
        sudo apt-get update -qq && sudo apt-get install -y jq curl
        pip install --upgrade pip
        pip install -r src/backend/requirements.txt
    
    - name: ðŸ“¥ Download collected articles
      uses: actions/download-artifact@v4
      with:
        name: collected-articles
        path: ./
    
    - name: ðŸ¤– Run Groq AI processing pipeline
      id: process
      env:
        GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
      run: |
        echo "ðŸš€ Starting Groq AI processing pipeline..."
        echo "ðŸ“Š Processing ${{ needs.collect-news.outputs.article_count }} collected articles"
        max_attempts=3
        attempt=1
        
        while [ $attempt -le $max_attempts ]; do
          echo "ðŸ¤– Groq processing attempt $attempt/$max_attempts"
          
          # Clear any existing API files to ensure fresh content
          rm -f src/backend/api/latest.json src/frontend/api/latest.json
          echo "ðŸ§¹ Cleared previous API files for fresh content"
          
          # Create a minimal .env.local for the orchestrator
          echo "GROQ_API_KEY=$GROQ_API_KEY" > .env.local
          
          # Run the Groq processing orchestrator with collected articles
          if python src/backend/processors/orchestrator.py test_output/collected_articles.json; then
            echo "âœ… Groq processing pipeline successful"
            
            # Clean up the temporary .env file
            rm -f .env.local
            
            # Verify the orchestrator generated the expected outputs
            if [ -f "src/backend/api/latest.json" ] && [ -f "src/frontend/api/latest.json" ]; then
              echo "âœ… Fresh AI-processed API endpoints generated"
              
              # Verify timestamp is recent (within last hour)
              generated_time=$(jq -r '.generated_at' src/frontend/api/latest.json)
              echo "ðŸ“… Content generated at: $generated_time"
              
              echo "âœ… Groq processing pipeline artifacts ready"
            else
              echo "âš ï¸ API endpoints missing, Groq pipeline may have issues"
            fi
            break
          else
            echo "âŒ Groq processing attempt $attempt failed"
            # Clean up on failure too
            rm -f .env.local
            if [ $attempt -eq $max_attempts ]; then
              echo "ðŸš¨ All Groq processing attempts failed"
              exit 1
            fi
            sleep 60  # Longer sleep for Groq rate limits
            ((attempt++))
          fi
        done
    
    - name: ðŸ” Validate Groq processing results
      id: verify
      run: |
        echo "ðŸ” Validating Groq processing results..."
        
        # Check for processed articles (main output)
        if [ ! -f "src/frontend/api/latest.json" ]; then
          echo "âŒ Frontend API latest.json missing!"
          ls -la src/frontend/api/ || echo "Frontend API directory doesn't exist"
          exit 1
        fi
        
        if [ ! -f "src/backend/api/latest.json" ]; then
          echo "âŒ Backend API latest.json missing!"
          ls -la src/backend/api/ || echo "Backend API directory doesn't exist"
          exit 1
        fi
        
        # Validate JSON structure
        if ! jq empty src/frontend/api/latest.json; then
          echo "âŒ Invalid JSON structure in frontend latest.json"
          exit 1
        fi
        
        if ! jq empty src/backend/api/latest.json; then
          echo "âŒ Invalid JSON structure in backend latest.json"
          exit 1
        fi
        
        # Get article counts from the processed results
        article_count=$(jq '.articles | length // 0' src/frontend/api/latest.json)
        pipeline_version=$(jq -r '.pipeline_info.version // "unknown"' src/frontend/api/latest.json)
        processing_time=$(jq '.pipeline_info.processing_time // 0' src/frontend/api/latest.json)
        generated_at=$(jq -r '.generated_at' src/frontend/api/latest.json)
        
        echo "article_count=$article_count" >> $GITHUB_OUTPUT
        echo "pipeline_version=$pipeline_version" >> $GITHUB_OUTPUT
        echo "âœ… Groq Processing Results Validated:"
        echo "   - Articles: $article_count (AI-processed content)"
        echo "   - Generated: $generated_at"
        echo "   - Pipeline: $pipeline_version"
        echo "   - Processing Time: ${processing_time}s"
        
        if [ "$article_count" -lt 1 ]; then
          echo "âš ï¸ Warning: No articles were processed through Groq pipeline"
        else
          echo "ðŸŽ‰ Successfully processed $article_count articles with Groq AI"
        fi
    
    - name: ðŸ’¾ Commit AI-processed content
      id: commit
      run: |
        git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        # Ensure usage reports are NOT committed to public repo
        rm -rf test_output/ logs/ || true
        echo "ðŸ”’ Removed private usage reports from commit (available in artifacts)"
        
        git add src/backend/data/ src/backend/api/ src/frontend/api/
        
        if git diff --staged --quiet; then
          echo "â„¹ï¸ No new content changes to commit"
          echo "has_changes=false" >> $GITHUB_OUTPUT
        else
          # Get article count from the processed API endpoint
          if [ -f "src/frontend/api/latest.json" ]; then
            article_count=$(jq '.articles | length // 0' src/frontend/api/latest.json)
            pipeline_version=$(jq -r '.pipeline_info.version // "unknown"' src/frontend/api/latest.json)
            commit_msg="ðŸ¤– Groq AI Update: $article_count articles ($pipeline_version) - $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          else
            commit_msg="ðŸ¤– Groq AI Update - $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
          fi
          
          git commit -m "$commit_msg"
          git push
          
          echo "has_changes=true" >> $GITHUB_OUTPUT
          echo "âœ… AI-processed content committed and pushed"
          echo "ðŸŒ Users will see updated articles on next site visit"
        fi
    
    - name: ðŸ“¤ Upload processed articles (Public Data)
      uses: actions/upload-artifact@v4
      with:
        name: processed-articles
        path: |
          src/backend/data/
          src/backend/api/
          src/frontend/api/
        retention-days: 7

    - name: ðŸ”’ Upload Groq usage reports (Owner Only)
      uses: actions/upload-artifact@v4
      with:
        name: groq-usage-reports
        path: |
          test_output/groq_api_usage.json
          test_output/groq_usage_summary.txt
          test_output/cost_estimates.json
        retention-days: 30

  # ================================================================
  # JOB 3: FRONTEND DEPLOYMENT - Deploy Fresh Content to Users
  # ================================================================
  deploy-frontend:
    name: ðŸŒ Deploy Fresh News Content to GitHub Pages
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: groq-processing
    if: success() && (needs.groq-processing.outputs.has_changes == 'true' || github.event.inputs.force_refresh == 'true' || github.event_name == 'workflow_dispatch')
    
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
    - name: ðŸ“¦ Checkout repository
      uses: actions/checkout@v4
      with:
        ref: ${{ github.ref }}
        
    - name: ðŸ“¥ Download processed articles
      uses: actions/download-artifact@v4
      with:
        name: processed-articles
        path: processed-artifacts/
        
    - name: ðŸ—ï¸ Prepare fresh content for deployment
      run: |
        echo "ðŸ—ï¸ Preparing fresh news content for user deployment..."
        echo "ðŸ”„ This will overwrite previous content with latest articles"
        
        # Create clean deployment directory
        mkdir -p site/
        
        # Copy frontend static files (includes fresh API data)
        echo "ðŸ“‹ Copying frontend assets with fresh news API..."
        cp -r src/frontend/* site/
        
        # Copy API data from processed artifacts (ensures fresh content availability)
        echo "ðŸ”„ Integrating AI-processed API data..."
        mkdir -p site/api
        if [ -d "processed-artifacts" ]; then
          cp -r processed-artifacts/* site/api/ 2>/dev/null || true
        fi
        
        # Handle Google Analytics injection
        if [ -f "site/index.html" ]; then
          if [ -n "${{ secrets.GOOGLE_ANALYTICS_ID }}" ]; then
            echo "ðŸ“Š Injecting Google Analytics..."
            sed -i "s|__GOOGLE_ANALYTICS__|${{ secrets.GOOGLE_ANALYTICS_ID }}|g" site/index.html
          else
            echo "ðŸ§¹ Removing Google Analytics placeholder..."
            sed -i 's|__GOOGLE_ANALYTICS__||g' site/index.html
          fi
        fi
        
        echo "âœ… Fresh content prepared for users"

    - name: ðŸ” Validate fresh content deployment
      run: |
        echo "ðŸ” Validating fresh content deployment..."
        
        # Critical file checks
        if [ ! -f "site/index.html" ]; then
          echo "âŒ index.html missing!"
          find site -type f | sort
          exit 1
        fi
        
        # Check API endpoints contain fresh content
        api_files=$(find site/api -name "*.json" 2>/dev/null | wc -l)
        if [ "$api_files" -eq 0 ]; then
          echo "âš ï¸ Warning: No fresh API files found"
        else
          echo "âœ… Fresh API endpoints: $api_files JSON files"
          
          # Check if latest.json has recent timestamp
          if [ -f "site/api/latest.json" ]; then
            generated_at=$(jq -r '.generated_at // "unknown"' site/api/latest.json)
            article_count=$(jq '.articles | length // 0' site/api/latest.json)
            echo "ðŸ“° Fresh content: $article_count articles generated at $generated_at"
          fi
        fi
        
        total_files=$(find site -type f | wc -l)
        echo "âœ… Fresh content deployment validated: $total_files files ready"
        echo "ðŸŒ Users will see updated articles when they visit/refresh the site"
        
        if [ "${{ github.event.inputs.debug }}" = "true" ]; then
          echo "=== Debug: Site structure ==="
          find site -type f | sort
        fi

    - name: ðŸ“¤ Upload Pages artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: site
    
    - name: ðŸš€ Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4
      with:
        timeout: 600000

  # ================================================================
  # JOB 4: WORKFLOW SUMMARY & MONITORING
  # ================================================================
  summary:
    name: ðŸ“Š Workflow Summary
    runs-on: ubuntu-latest
    needs: [collect-news, groq-processing, deploy-frontend]
    if: always()
    
    steps:
    - name: ðŸ“‹ Generate workflow summary
      run: |
        echo "# ðŸ“° NewsXP AI - Multi-Stage AI News Pipeline" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "ðŸ”„ **Multi-Stage Architecture**: This workflow separates news collection from AI processing for better reliability." >> $GITHUB_STEP_SUMMARY
        echo "ðŸ“± **User Experience**: Fresh AI-processed content delivered to users." >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## ðŸ”§ Configuration" >> $GITHUB_STEP_SUMMARY
        echo "- **Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Sources:** ${{ github.event.inputs.sources || 'all' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Force Refresh:** ${{ github.event.inputs.force_refresh || 'false' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Debug Mode:** ${{ github.event.inputs.debug || 'false' }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "## ðŸ“Š Pipeline Results" >> $GITHUB_STEP_SUMMARY
        
        # Collection Results
        if [ "${{ needs.collect-news.result }}" = "success" ]; then
          echo "âœ… **News Collection:** ${{ needs.collect-news.outputs.article_count }} raw articles collected" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **News Collection:** Failed - check collection logs" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Groq Processing Results
        if [ "${{ needs.groq-processing.result }}" = "success" ]; then
          if [ "${{ needs.groq-processing.outputs.has_changes }}" = "true" ]; then
            echo "âœ… **Groq AI Processing:** ${{ needs.groq-processing.outputs.article_count }} articles processed" >> $GITHUB_STEP_SUMMARY
            echo "âœ… **Pipeline Version:** ${{ needs.groq-processing.outputs.pipeline_version || '4-stage-optimized' }}" >> $GITHUB_STEP_SUMMARY
            echo "ðŸ¤– **AI Status:** Articles enhanced with AI curation and analysis" >> $GITHUB_STEP_SUMMARY
          else
            echo "â„¹ï¸ **Groq Processing:** No new processed articles this cycle" >> $GITHUB_STEP_SUMMARY
          fi
        elif [ "${{ needs.groq-processing.result }}" = "skipped" ]; then
          echo "â­ï¸ **Groq Processing:** Skipped (no articles collected)" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Groq Processing:** Failed - check AI pipeline logs" >> $GITHUB_STEP_SUMMARY
        fi
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "## ðŸŒ User Content Deployment" >> $GITHUB_STEP_SUMMARY
        if [ "${{ needs.deploy-frontend.result }}" = "success" ]; then
          echo "âœ… **Content Deployed:** Successfully updated GitHub Pages" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ”— **User Access:** Latest AI-processed articles available immediately" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“± **Refresh Behavior:** Users see new content on page refresh/revisit" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ needs.deploy-frontend.result }}" = "skipped" ]; then
          echo "â­ï¸ **Deployment:** Skipped (no processed content or disabled)" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Deployment:** Failed - users may see stale content" >> $GITHUB_STEP_SUMMARY
        fi
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "## ðŸ—ï¸ Multi-Stage Architecture" >> $GITHUB_STEP_SUMMARY
        echo "This workflow implements a **3-job pipeline architecture**:" >> $GITHUB_STEP_SUMMARY
        echo "1. **ðŸ“° News Collection** - Gather raw articles from sources" >> $GITHUB_STEP_SUMMARY
        echo "2. **ðŸ¤– Groq AI Processing** - Multi-stage AI pipeline for content curation" >> $GITHUB_STEP_SUMMARY
        echo "3. **ðŸŒ Frontend Deployment** - Deploy processed content to GitHub Pages" >> $GITHUB_STEP_SUMMARY
        echo "4. **ðŸ“Š Monitoring** - Track pipeline health and performance" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Benefits of Separation:**" >> $GITHUB_STEP_SUMMARY
        echo "- Independent scaling of collection vs processing" >> $GITHUB_STEP_SUMMARY
        echo "- Better error isolation and debugging" >> $GITHUB_STEP_SUMMARY
        echo "- Separate Groq API rate limiting and monitoring" >> $GITHUB_STEP_SUMMARY
        echo "- Artifact-based data flow between stages" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.collect-news.result }}" != "success" ] || [ "${{ needs.groq-processing.result }}" = "failure" ] || [ "${{ needs.deploy-frontend.result }}" = "failure" ]; then
          echo "## ðŸš¨ Troubleshooting" >> $GITHUB_STEP_SUMMARY
          echo "Check individual job logs for detailed error information." >> $GITHUB_STEP_SUMMARY
          echo "- News collection issues: Check source connectivity" >> $GITHUB_STEP_SUMMARY
          echo "- Groq processing issues: Check API key and rate limits" >> $GITHUB_STEP_SUMMARY
          echo "- Deployment issues: Ensure GitHub Pages is configured properly" >> $GITHUB_STEP_SUMMARY
        fi